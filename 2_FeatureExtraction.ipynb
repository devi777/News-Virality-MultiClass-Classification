{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "2_FeatureExtraction.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devi777/News-Virality-MultiClass-Classification/blob/master/2_FeatureExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV6Ekmulr9Wi",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we are going to pre-process the data, apply One-Hot Encoding to the target(y), separate the new columns, apply Dimensionality Reduction using Feature Extraction and apply Logistic Regression on each column, then combine the predictions, Reverse One-Hot Encode them, and then find their accuracies using Confusion Matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFRfnQQ0rGvU",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKFoKO8trGvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea3eEhDfrGvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the dataset\n",
        "dataset = pd.read_csv('News3.csv')\n",
        "X = dataset.iloc[:, 1:-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsqF_Z_NrGvh",
        "colab_type": "code",
        "colab": {},
        "outputId": "7905feff-d279-406f-a334-2a77658c59f8"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NAME</th>\n",
              "      <th>VPM</th>\n",
              "      <th>KOB</th>\n",
              "      <th>DPPV</th>\n",
              "      <th>DTS</th>\n",
              "      <th>BR</th>\n",
              "      <th>TP</th>\n",
              "      <th>TR</th>\n",
              "      <th>PB</th>\n",
              "      <th>PPD</th>\n",
              "      <th>AR</th>\n",
              "      <th>SEM</th>\n",
              "      <th>TFPM</th>\n",
              "      <th>DA</th>\n",
              "      <th>TARGET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>YahooNews</td>\n",
              "      <td>175000000</td>\n",
              "      <td>2.800</td>\n",
              "      <td>4.34</td>\n",
              "      <td>4.600</td>\n",
              "      <td>39.2</td>\n",
              "      <td>8.3</td>\n",
              "      <td>0.228022</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25</td>\n",
              "      <td>11</td>\n",
              "      <td>395</td>\n",
              "      <td>1.10</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HuffingtonPost</td>\n",
              "      <td>110000000</td>\n",
              "      <td>1.200</td>\n",
              "      <td>1.50</td>\n",
              "      <td>2.260</td>\n",
              "      <td>48.1</td>\n",
              "      <td>20.8</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>29</td>\n",
              "      <td>11655</td>\n",
              "      <td>4500</td>\n",
              "      <td>11.40</td>\n",
              "      <td>93</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CNN</td>\n",
              "      <td>95000000</td>\n",
              "      <td>1.100</td>\n",
              "      <td>2.28</td>\n",
              "      <td>4.230</td>\n",
              "      <td>52.3</td>\n",
              "      <td>29.1</td>\n",
              "      <td>0.659864</td>\n",
              "      <td>1.0</td>\n",
              "      <td>30</td>\n",
              "      <td>72</td>\n",
              "      <td>417</td>\n",
              "      <td>47.20</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NewYorkTimes</td>\n",
              "      <td>70000000</td>\n",
              "      <td>1.700</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3.150</td>\n",
              "      <td>60.6</td>\n",
              "      <td>41.9</td>\n",
              "      <td>0.945824</td>\n",
              "      <td>1.0</td>\n",
              "      <td>28</td>\n",
              "      <td>77</td>\n",
              "      <td>555</td>\n",
              "      <td>44.70</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>FoxNews</td>\n",
              "      <td>65000000</td>\n",
              "      <td>0.550</td>\n",
              "      <td>2.59</td>\n",
              "      <td>5.210</td>\n",
              "      <td>46.3</td>\n",
              "      <td>22.2</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30</td>\n",
              "      <td>227</td>\n",
              "      <td>15</td>\n",
              "      <td>18.50</td>\n",
              "      <td>95</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NBCNews</td>\n",
              "      <td>63000000</td>\n",
              "      <td>0.460</td>\n",
              "      <td>1.37</td>\n",
              "      <td>2.260</td>\n",
              "      <td>74.3</td>\n",
              "      <td>46.6</td>\n",
              "      <td>1.290859</td>\n",
              "      <td>1.0</td>\n",
              "      <td>30</td>\n",
              "      <td>507</td>\n",
              "      <td>1000</td>\n",
              "      <td>7.50</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>MailOnline</td>\n",
              "      <td>53000000</td>\n",
              "      <td>0.710</td>\n",
              "      <td>2.04</td>\n",
              "      <td>4.350</td>\n",
              "      <td>59.1</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.631164</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30</td>\n",
              "      <td>271</td>\n",
              "      <td>16</td>\n",
              "      <td>16.40</td>\n",
              "      <td>94</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>TheGuardian</td>\n",
              "      <td>42000000</td>\n",
              "      <td>1.400</td>\n",
              "      <td>2.64</td>\n",
              "      <td>2.683</td>\n",
              "      <td>62.7</td>\n",
              "      <td>52.7</td>\n",
              "      <td>1.203196</td>\n",
              "      <td>1.0</td>\n",
              "      <td>30</td>\n",
              "      <td>119</td>\n",
              "      <td>50</td>\n",
              "      <td>8.20</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ABCNews</td>\n",
              "      <td>36000000</td>\n",
              "      <td>0.482</td>\n",
              "      <td>1.39</td>\n",
              "      <td>2.330</td>\n",
              "      <td>70.8</td>\n",
              "      <td>45.2</td>\n",
              "      <td>1.231608</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25</td>\n",
              "      <td>1047</td>\n",
              "      <td>1230</td>\n",
              "      <td>0.05</td>\n",
              "      <td>93</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>TimesOfIndia</td>\n",
              "      <td>212000000</td>\n",
              "      <td>0.063</td>\n",
              "      <td>1.40</td>\n",
              "      <td>2.050</td>\n",
              "      <td>64.1</td>\n",
              "      <td>46.2</td>\n",
              "      <td>0.540351</td>\n",
              "      <td>0.5</td>\n",
              "      <td>26</td>\n",
              "      <td>18024</td>\n",
              "      <td>467</td>\n",
              "      <td>12.70</td>\n",
              "      <td>93</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NDTVNews</td>\n",
              "      <td>50000000</td>\n",
              "      <td>0.610</td>\n",
              "      <td>2.36</td>\n",
              "      <td>3.980</td>\n",
              "      <td>56.5</td>\n",
              "      <td>43.3</td>\n",
              "      <td>0.714521</td>\n",
              "      <td>1.0</td>\n",
              "      <td>30</td>\n",
              "      <td>358</td>\n",
              "      <td>11</td>\n",
              "      <td>12.60</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>IndiaToday</td>\n",
              "      <td>17000000</td>\n",
              "      <td>0.300</td>\n",
              "      <td>1.59</td>\n",
              "      <td>2.380</td>\n",
              "      <td>74.6</td>\n",
              "      <td>78.2</td>\n",
              "      <td>1.486692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30</td>\n",
              "      <td>567</td>\n",
              "      <td>70</td>\n",
              "      <td>5.30</td>\n",
              "      <td>89</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>IndianExpress</td>\n",
              "      <td>18000000</td>\n",
              "      <td>0.319</td>\n",
              "      <td>2.34</td>\n",
              "      <td>3.530</td>\n",
              "      <td>62.1</td>\n",
              "      <td>41.3</td>\n",
              "      <td>0.645312</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30</td>\n",
              "      <td>1100</td>\n",
              "      <td>7</td>\n",
              "      <td>3.50</td>\n",
              "      <td>91</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>TheHindu</td>\n",
              "      <td>65000000</td>\n",
              "      <td>0.253</td>\n",
              "      <td>1.91</td>\n",
              "      <td>2.750</td>\n",
              "      <td>65.8</td>\n",
              "      <td>64.4</td>\n",
              "      <td>1.047154</td>\n",
              "      <td>0.5</td>\n",
              "      <td>30</td>\n",
              "      <td>977</td>\n",
              "      <td>1</td>\n",
              "      <td>6.00</td>\n",
              "      <td>91</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              NAME        VPM    KOB  DPPV    DTS    BR    TP        TR   PB  \\\n",
              "0        YahooNews  175000000  2.800  4.34  4.600  39.2   8.3  0.228022  1.0   \n",
              "1   HuffingtonPost  110000000  1.200  1.50  2.260  48.1  20.8  0.500000  2.0   \n",
              "2              CNN   95000000  1.100  2.28  4.230  52.3  29.1  0.659864  1.0   \n",
              "3     NewYorkTimes   70000000  1.700  2.50  3.150  60.6  41.9  0.945824  1.0   \n",
              "4          FoxNews   65000000  0.550  2.59  5.210  46.3  22.2  0.500000  2.0   \n",
              "5          NBCNews   63000000  0.460  1.37  2.260  74.3  46.6  1.290859  1.0   \n",
              "6       MailOnline   53000000  0.710  2.04  4.350  59.1  32.0  0.631164  2.0   \n",
              "7      TheGuardian   42000000  1.400  2.64  2.683  62.7  52.7  1.203196  1.0   \n",
              "8         Â ABCNews   36000000  0.482  1.39  2.330  70.8  45.2  1.231608  1.0   \n",
              "9     TimesOfIndia  212000000  0.063  1.40  2.050  64.1  46.2  0.540351  0.5   \n",
              "10        NDTVNews   50000000  0.610  2.36  3.980  56.5  43.3  0.714521  1.0   \n",
              "11      IndiaToday   17000000  0.300  1.59  2.380  74.6  78.2  1.486692  0.0   \n",
              "12   IndianExpress   18000000  0.319  2.34  3.530  62.1  41.3  0.645312  0.0   \n",
              "13        TheHindu   65000000  0.253  1.91  2.750  65.8  64.4  1.047154  0.5   \n",
              "\n",
              "    PPD     AR   SEM   TFPM  DA  TARGET  \n",
              "0    25     11   395   1.10  95       0  \n",
              "1    29  11655  4500  11.40  93       1  \n",
              "2    30     72   417  47.20  95       0  \n",
              "3    28     77   555  44.70  95       0  \n",
              "4    30    227    15  18.50  95       1  \n",
              "5    30    507  1000   7.50  92       1  \n",
              "6    30    271    16  16.40  94       1  \n",
              "7    30    119    50   8.20  95       0  \n",
              "8    25   1047  1230   0.05  93       0  \n",
              "9    26  18024   467  12.70  93       1  \n",
              "10   30    358    11  12.60  92       1  \n",
              "11   30    567    70   5.30  89       2  \n",
              "12   30   1100     7   3.50  91       2  \n",
              "13   30    977     1   6.00  91       2  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftFQGUYzrGvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "onehotencoder = OneHotEncoder(categories = 'auto')\n",
        "y = np.reshape(y,(-1,1))\n",
        "y = onehotencoder.fit_transform(y).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZDaWxzprGvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 4/13, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Xm9rrTrGvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0QmqyoqrGvz",
        "colab_type": "text"
      },
      "source": [
        "# Dimensionality Reduction using Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SergbcMFrGvz",
        "colab_type": "text"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23lurh90rGv0",
        "colab_type": "text"
      },
      "source": [
        "Here, we first take n_components as None to find out the variances of the 13 new extracted components from the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxcrsttGrGv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#From explains_variance , we get to know that the top 2 principal components\n",
        "# (or independent variables), matter\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components = None)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "explains_variance = pca.explained_variance_ratio_ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIUPDd_YrGv5",
        "colab_type": "code",
        "colab": {},
        "outputId": "e8c390fb-6101-4322-eb07-b6875b59807d"
      },
      "source": [
        "print(explains_variance)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.18945118e-01 2.34143665e-01 1.50513797e-01 1.14142051e-01\n",
            " 4.86691736e-02 1.48826161e-02 1.29074493e-02 5.79613065e-03\n",
            " 3.64078025e-33]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCShry4MrGv_",
        "colab_type": "text"
      },
      "source": [
        "Now, as we can see, the top 2 components consitute 64% of the variance. Let's now, use only the top 2 components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikXkKgwlrGv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components = 2)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "explains_variance = pca.explained_variance_ratio_ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzxcAvmlrGwB",
        "colab_type": "code",
        "colab": {},
        "outputId": "84934490-7718-4b27-b253-cf72b7432925"
      },
      "source": [
        "print(explains_variance)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.41894512 0.23414367]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izYbsG5krGwE",
        "colab_type": "code",
        "colab": {},
        "outputId": "71ee8f50-ed0d-4c41-d1fd-91570ae013b6"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.53217796, -0.6228316 ],\n",
              "       [-0.17849992,  2.8365194 ],\n",
              "       [ 1.52267615,  3.38623617],\n",
              "       [-0.74471535, -1.14942804],\n",
              "       [-0.340227  , -1.15971083],\n",
              "       [ 0.59153469, -0.70558818],\n",
              "       [ 5.56652352, -1.47032506],\n",
              "       [-2.5344777 ,  0.32209743],\n",
              "       [-1.35063642, -1.43696928]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fj0Mq5irGwK",
        "colab_type": "text"
      },
      "source": [
        "So, in order to apply Multi class Linear Regression to each column of y, and then combine the 3 y_pred columns, compare values for each training example, an assign 1 to the one with maximum value, and 0 to others. Let's start again from train/test splitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xex_IUVXrGwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train0 = y_train[:,0]\n",
        "y_train1 = y_train[:,1]\n",
        "y_train2 = y_train[:,2]\n",
        "\n",
        "y_test0 = y_test[:,0]\n",
        "y_test1 = y_test[:,1]\n",
        "y_test2 = y_test[:,2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46SaHu2IrGwR",
        "colab_type": "code",
        "colab": {},
        "outputId": "90f986b3-63bb-439d-c7ad-497b26441792"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnpP61APrGwU",
        "colab_type": "code",
        "colab": {},
        "outputId": "4f782d2c-f2fa-4c2c-d1aa-5fa1707bfbc3"
      },
      "source": [
        "print(y_train,y_train0,y_train1,y_train2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]] [0. 0. 0. 1. 0. 1. 1. 0. 0.] [0. 1. 1. 0. 1. 0. 0. 1. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY7RQWGprGwW",
        "colab_type": "text"
      },
      "source": [
        "Now, let's apply Logistic Regression to each set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Bz3W-SrGwW",
        "colab_type": "text"
      },
      "source": [
        "###  Analyzing Data using Logistic Regression on Training Set 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2gD985IrGwY",
        "colab_type": "code",
        "colab": {},
        "outputId": "de976612-62d0-431c-b9ae-e39361c6d842"
      },
      "source": [
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier0 = LogisticRegression(random_state = 0)\n",
        "classifier0.fit(X_train, y_train0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG2soUaIrGwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred0 = classifier0.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38tVmEYDrGwi",
        "colab_type": "code",
        "colab": {},
        "outputId": "51f00092-84b4-4706-a06a-9c5c53d6ee04"
      },
      "source": [
        "# if y_pred0 = (y_pred0 > 0.5), then the accuracy would have been..\n",
        "print('Test accuracy {:.2f}%'.format(classifier0.score(X_test,y_test0)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy 40.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMdusvxUrGwk",
        "colab_type": "text"
      },
      "source": [
        "###  Analyzing Data using  Logistic Regression on Training Set 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjwfK5DSrGwl",
        "colab_type": "code",
        "colab": {},
        "outputId": "a357723d-bf07-442e-f869-fce24df02876"
      },
      "source": [
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier1 = LogisticRegression(random_state = 0)\n",
        "classifier1.fit(X_train, y_train1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIb9pwfOrGwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred1 = classifier1.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcJ4rf3qrGwp",
        "colab_type": "code",
        "colab": {},
        "outputId": "92acd32c-68f3-4140-bcf1-3250e033be44"
      },
      "source": [
        "# if y_pred1 = (y_pred1 > 0.5), then the accuracy would have been..\n",
        "print('Test accuracy {:.2f}%'.format(classifier1.score(X_test,y_test1)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy 40.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idDFdFHcrGwt",
        "colab_type": "text"
      },
      "source": [
        "### Analyzing Data using Logistic Regression on Training Set 3 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekYld8DCrGwt",
        "colab_type": "code",
        "colab": {},
        "outputId": "fd6e645d-124a-4e9e-f810-4d84eec4c93b"
      },
      "source": [
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier2 = LogisticRegression(random_state = 0)\n",
        "classifier2.fit(X_train, y_train2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlWtSN9erGwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred2 = classifier2.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrRj2a_rrGwx",
        "colab_type": "code",
        "colab": {},
        "outputId": "499ae544-2f2d-4bc8-efb8-2d98d520f6f2"
      },
      "source": [
        "# if y_pred2 = (y_pred2 > 0.5), then the accuracy would have been..\n",
        "print('Test accuracy {:.2f}%'.format(classifier2.score(X_test,y_test2)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGArXs4frGw2",
        "colab_type": "text"
      },
      "source": [
        "### Combining and Reverse One-Hot Encoding the predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHssy-VFrGw3",
        "colab_type": "text"
      },
      "source": [
        "So, what I am here going to do is.. combine all the (y_pred) predicted values and actual values (y_test) that I did for the high, low, and medium categories separately into a single numpy array y_pred, and then "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX4lULUIrGw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_pred = np.array([y_pred0.astype(int), y_pred1.astype(int), y_pred2.astype(int)])\n",
        "y_pred = np.array([y_pred0, y_pred1, y_pred2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKxVzYprGw5",
        "colab_type": "code",
        "colab": {},
        "outputId": "c6010beb-d314-4310-c5a7-7963607c2787"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., 0., 1.],\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIinQ8pUrGxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tranposing the y_pred so that it's shape remains same to that of y_test\n",
        "y_pred = y_pred.transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8smtQ_-HrGxC",
        "colab_type": "code",
        "colab": {},
        "outputId": "3da54a3e-1e4e-482c-9290-62ae2778b685"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFDqhq0orGxH",
        "colab_type": "code",
        "colab": {},
        "outputId": "f3e372f7-e444-47d9-80b9-1bb11d8fbd6f"
      },
      "source": [
        "#Printing a multilabel confusion matrix to check the results\n",
        "import sklearn.metrics as skm\n",
        "cm = skm.multilabel_confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print( skm.classification_report(y_test,y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1 2]\n",
            "  [1 1]]\n",
            "\n",
            " [[2 1]\n",
            "  [2 0]]\n",
            "\n",
            " [[4 0]\n",
            "  [0 1]]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.50      0.40         2\n",
            "           1       0.00      0.00      0.00         2\n",
            "           2       1.00      1.00      1.00         1\n",
            "\n",
            "   micro avg       0.40      0.40      0.40         5\n",
            "   macro avg       0.44      0.50      0.47         5\n",
            "weighted avg       0.33      0.40      0.36         5\n",
            " samples avg       0.40      0.40      0.40         5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mNd_iEPrGxJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "372b1dea-ed33-4b40-8623-66cc8a622aaf"
      },
      "source": [
        "y_pred1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H4fO_nqrGxP",
        "colab_type": "code",
        "colab": {},
        "outputId": "0c776714-119b-4fe5-f062-e7c9031e3070"
      },
      "source": [
        "# Converting the y_pred numpy array to list , for convenience\n",
        "y_pred_list = y_pred.tolist()\n",
        "print(y_pred_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNGx_3torGxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a categorised array with zeroes by equating it with the the newly created list \n",
        "# And initalizing it to the index which has value 1.0  \n",
        "y_pred_categorised = np.zeros(np.array(y_pred_list).shape[0])\n",
        "for i in range(len(y_pred_list)):\n",
        "    for j in range(len(y_pred_list[i])):\n",
        "        if y_pred[i][j] == 1.0:\n",
        "            y_pred_categorised[i] = j "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE7QcEYLrGxV",
        "colab_type": "code",
        "colab": {},
        "outputId": "14c636f2-ed22-4c70-b994-2e0b98ffda6c"
      },
      "source": [
        "y_pred_categorised"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 2., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz9_K1TKrGxX",
        "colab_type": "code",
        "colab": {},
        "outputId": "21a29316-9b35-49b1-caa8-9bca3b701379"
      },
      "source": [
        "# Same process for y_test\n",
        "y_test_list = y_test.tolist()\n",
        "print(y_test_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmEsxQNmrGxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_categorised = np.zeros(np.array(y_test_list).shape[0])\n",
        "for i in range(len(y_test_list)):\n",
        "    for j in range(len(y_test_list[i])):\n",
        "        if y_pred[i][j] == 1.0:\n",
        "            y_test_categorised[i] = j "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIzVXmwsrGxf",
        "colab_type": "code",
        "colab": {},
        "outputId": "f71f1e6c-fd5d-4bc0-f041-ddca4e4bc03b"
      },
      "source": [
        "y_test_categorised"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 2., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G75iY7yrrGxh",
        "colab_type": "code",
        "colab": {},
        "outputId": "ed1f2078-9c9a-4fe1-eef1-e17a4cca99ea"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfBfNE4srGxj",
        "colab_type": "code",
        "colab": {},
        "outputId": "124def06-377c-4253-e12a-1938872c800f"
      },
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test_categorised,y_pred_categorised)\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9jBBK1trGxo",
        "colab_type": "code",
        "colab": {},
        "outputId": "5eb1efdc-762a-4214-becf-01e9082b54e1"
      },
      "source": [
        "print(\"Accuracy:\",str(round((3+1+1)/(5)*100,2)), \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 100.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbJ_i3WprGxv",
        "colab_type": "text"
      },
      "source": [
        "Wow, we got a 100% accuracy on our model. So to summarize, we applied Dimensionality Reduction using Feature Extraction using PCA, and then applying Logistic Regression to each category separately, and then Reverse One Hot Encoded the predicted and actual values to print the confusion matrix and check our accuracy. Let's try to plot them as well :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3pBugctrGxw",
        "colab_type": "text"
      },
      "source": [
        "### Data Visualization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "6UMbLK7VrGxx",
        "colab_type": "code",
        "colab": {},
        "outputId": "d99689bd-2aa1-4470-d80e-30dec1fb7a7b"
      },
      "source": [
        "# Visualising the Training set results\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set0 = X_train, y_train0\n",
        "y_set1, y_set2 = y_train1, y_train2\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, classifier0.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green','blue')))\n",
        "plt.contourf(X1, X2, classifier1.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green','blue')))\n",
        "plt.contourf(X1, X2, classifier2.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green','blue')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set0 == j, 0], X_set[y_set0 == j, 1],\n",
        "                c = ListedColormap(('red', 'green','blue'))(i), label = j)\n",
        "    plt.scatter(X_set[y_set1 == j, 0], X_set[y_set1 == j, 1],\n",
        "                c = ListedColormap(('red', 'green','blue'))(i), label = j)\n",
        "    plt.scatter(X_set[y_set2 == j, 0], X_set[y_set2 == j, 1],\n",
        "                c = ListedColormap(('red', 'green','blue'))(i), label = j)\n",
        "plt.title('Logistic Regression (Training set)')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
            "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
            "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
            "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
            "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZwcdbnv8c8z3ZlMtpkBgQQygQhBTAwQICgICiQcNgnb8XBRXI/HXA/LwegVL2BAUMTlHhBXbgSPG0cuIqhhUTAYIB4VEgyQGOOJQMiQkBDIJBkyWbrnuX9UdTJL9/RMb9Xd9X2/XvPKTFV11W86Pc9T9fs99Stzd0REJH4aom6AiIhEQwlARCSmlABERGJKCUBEJKaUAEREYkoJQEQkppQApBczu9jMHi7wtcvN7OQSN6nqmdlDZvbhMu37NDP7RZn2fbCZdZZ626iY2f5m9hcza4y6LbXCdB9A7TKzF4F/cfffRnDsHwDt7v65IvczEXgBeCNctBG4zd2/XMx+64WZLQYuA9YCf+mxahSwDcj8AZ/p7k9UuHmRMrN24APuvrDHsnnAn939u5E1rIYko26ASKjV3VNmNh14zMyWuPsjpTyAmSXdPVXKfZaTmR0LtLj7H8NFo3usc+BId181wOsT7p4uczOrzZ3ArYASwCCoC6hOmdnHzWyVmb1uZr8yswN6rDvNzFaa2WYz+46ZPWZm/xKu+4iZLQq/NzO7xcw2hNs+a2ZTzWw2cDFwpZl1mtn8cPsXzezU8PuEmV1tZn83s61mtsTMJuRrt7svBpYD03q09wAz+7mZvWpmL5jZv/VYN8LMfmhmm8xshZldGZ4ZZta/aGafNbNngTfMLJlnf283s8VmtsXM1pvZzeHyJjP7iZm9ZmYdZvaUmY0N1y3s8f41mNnnzGx1+L79yMxawnUTzczN7MNm9pKZbTSzawZ4O84EHsv3nvVo+0/M7Ntm9mszewN4l5mdY2ZLw/+Dl8xsbo/tJ4WJJPPzIjO73sz+K9z+12a291C3Ddd/tMfveLWZtVuO7kEzOzv8v9sabjenx7pzzOyZ8D1fZGZTw+U/BQ4AHgo/g58KX/IH4K1mNn6w71usubu+avQLeBE4NcvyGQRdKUcDw4FvAo+H6/YBtgAXEFwBXgHsIuhKAvgIsCj8/nRgCdAKGDAZ2D9c9wPgi7naA3wGeA44LHztkcCbsrR1IkE3RjL8+TiCro3zw58bwjZcCzQCBwPPA6eH679MECT3AtqAZwm6pnq2aSkwARgxiP39Afhg+P1o4Ljw+/8JzAdGAgngGKA5XLewx/v3z8CqcL+jgXuBH/f5Xb8XtuVIYAcwOcf/78+Az+RY58CkPst+AmwCjg9/z+HhZ2Fq+POR4efi7HD7SYD3eP0i4L+BQ8Pf84nM//EQtz0c2Aq8M2zDLUAKODnH7/Iq8M7w+72Bo8PvjwXWh/8mwvf270BjuL492z4JusrOivrvsxa+dAVQny4Gvu/uT7v7DuAq4HgL+tvPApa7+70edId8A3glx352AWOAtxKMF61w93WDbMO/AJ9z95UeeMbdXxtg+41m1kUQgL8DZAY+jwX2dfcb3H2nuz9PEEAvCtdfCHzJ3Te5e3v4+/T1DXdf4+5dg9jfLmCSme3j7p2+p/tlF/AmgqCbdvcl7r4ly7EuBm529+fdvZPgvb/IzHp2t17v7l3u/gzwDEFgzqaVIJAOxX3u/gd373b3He7+qLsvC39+BrgLOGmA19/h7v/t7tsIEtC0Arb9J+AX7v5f4ecv3zjRLmCKmY1x99fd/elw+WzgO+7+VPiefz9cfmye/W0leO8kDyWA+nQAsDrzQxiIXgPGh+vW9FjnBGdS/bj7o8C3gG8D681snpk1D7INEwjO1gZrH4Iz5v8FnAwMC5cfBBwQdgF0mFkHcDUwNlzf6/fp8322Zfn29zHgLcBfw26es8PlPwZ+A9xlZmvN7KtmNoz+er334ffJHvuH3gl3Gz369vvYRJCAh6LX729mx4ddVK+a2WaCxLzPAK8fbNsG2rbvZ+wNgt8ll/OBc4CXwra+I1x+EPDZPv9X+xN8jgcyBujIs42gBFCv1hL88QBgZqMIzl5fBtYRdJVk1lnPn/ty92+4+zHA2wgC42cyq/K0YQ1wyFAaHZ7l/TuwHbikx35ecPfWHl9j3P2scH2v34cg8fTbdZ925dxfeEb7PmA/4CvAPWY2yt13ufv17j6FoGvjbOBDWY7V670HDiTo/lg/hLci41mC93wo+v6/3AX8HJjg7i3A7QRdcuXU9zM2iqCLLit3/5O7n0Pwnt9P0GYI/q+u7/N/NdLd7868tO++LCgBPZjgykryUAKofcPCAcrMVxL4T+CjZjbNzIYDXwL+5O4vAg8Ah5vZeeG2lwLjsu3YzI41s3eEZ7pvEATmTFXJeoI/tFxuB75gZoda4Agze9Mgf6cvEwwwNwFPAlssGMgdYcHg8lQLKmQA7gauMrO9woG/y/Lse8D9mdkHzGxfd+9mz1lk2sxOMbPDzSxBMIayq8d70dNPgTlm9mYzG03w3v8/L6z66EEG7q4ZjDHA6+6+3cyOY09XVzn9DDjPzI4LA/INuTYM/w/eb2bN7r6LoPsm877OAy4NP4dmZqPNbFaYUCD7Z/A44G/u/nJpf6X6pARQ+x4Eunp8fd7dFwBzCc781hGciV8E4O4bCfpov0rQLTQFWEwwGNlXM0H/+CaCrozXgP8TrruDoN+2w7LfqHQzQXB+mCBg3kEw8DkYD4TH/LgHZYyzCPqXXyAYxLwdaAm3vYGgC+sF4LfAPTl+FyC4ysizvzOA5Rbc9HQrcJG7bydIkveEv8sKgoHnn2Q5xPcJuoseD/e/Hbh8kL9337Y+DWzu0SVSiH8FbjKzrQRdXXfn2b5o7v4sMIcgEawl+Ny8Ru7/lw8Dq81sC0EX3AfD/fyJoP3fJfg8/A34QI/XfQm4PvwMfjJcdjFwW0l/oTqmG8FizswaCALoxe7+u6jbUywz+1eCoF3smXNVMLPTgEvc/byo21KocNyoAzjI3bON0ZTqOPsDC4Bp7r6zXMepJ7oCiCEzO93MWsPuoasJ+oT/mOdlVcmC2/9PsKD+/jDg08B9UberVNz94VoM/mH9/siwG+zfgafLGfwB3H2du09R8B88JYB4Op6gQmcjQXfIeWGJZC1qBP4vQd/xo8AvCcpIJVrnE3T/tBPc//C+SFsjWakLSEQkpnQFICISUzU1GVyrJf0AhkfdDBGJmXR4rlzuGyjKZSWdG919377LayoBHMBw7kxOiboZIhIjHekm8ATJZK2Gf3h3auHqbMtrKgGIiFRSRyq456yWg/9ANAYgIpJFvQd/UAIQEeknDsEf1AUkIjHne42hYe5HsEnjwYxuLJxLusaCv3fTvWod275wF2wa3OOblQBEJNYa5n6Efd9+BK3JYXRjuCewGov9AI6zee83sWHuRWz71O2Deo26gEQk1mzS+JoP/gCG0ZIcTsOk/Qf9Gl0BiEi8mZHGoIaDf4ZhYIM/r4/8CiCcj/3PZnZ/1G0RkfjprpPgX4jIEwDBQ8lXRN0IEYmfTLVPNQT/R/7rCY6+4AyOPO80bv7BvH7rd+zcyUeumsOR553GKR++kNVrsz7JdUgiTQBm1ga8h+CBHCIiEYg++qfTaT79lRv4+Te+x1M/u597fvMAf31+Va9tfvTLe2gd08wzv3iYS9//Ya775r8XfdyorwC+DlwJdOfawMxmm9liM1u8iUKeqici0kc6TUdqFFbAqX/yofmMOnsGo4+dzKizZ5B8aH7RzVm8/FkOnnAgb26bQOOwRv7xtLN44LEFvbZ54LEFvO/s4NEQ5808nYVP/oFiZ3OOLAGY2dnABndfMtB27j7P3ae7+/S9NGYtIkXqbEjR4c0kk0YiMbTXJh+aT9ONc2l4ZS3mTsMra2m6cW7RSWDdhvW0jd1TvXPAfuNYu2F9n2027N4mmUzSPHoMr2/uoBhRXgGcAJxjZi8CdwEzzCzbM1ZFREpicyJFamdLwXf4Dv/2Ldj27b2W2fbtDP/2LUW1K9t5fN+rE8+6VXEiSwDufpW7t7n7RIIHlj/q7h/I8zIRkYJ0JFL4jsKDP4CtXzek5YN1wH5jae+xj7UbXmH/fffLuU0qlWJL51b2bmkt6rhRjwGIiFTGjpaC+vx78rHZb7LKtXywjplyOM+vWc2LL7ezc9dOfv7wg5z17hm9tjnr3TP46f2/AOAXC37DScceV/TvUxUJwN0XuvvZUbdDROpTptxzqH3+fe24dA7e1NRrmTc1sePSOUXtN5lM8rXPzOX8yz/G9Pe+h/NPPZPJhxzKF2/7Bg8+9igAHzr3vby+uYMjzzuNb935Az5/2aeLOibU2DOBp9go1wNhRGQo8s3sOXr+dUzadwjTJzw0PxgLWL8OH7s/Oy6dQ+rMWSVpaymsenUdnbOu77Xs3amFS9x9et9tVVYjIvUpnabDm4HSTuucOnNWVQX8YigBiEhd6vBmzIZe6hknVTEGICJSSqXq8693SgAiUlfi8jSvUlACEJG6oeA/NEoAIlIXFPyHTglARGpePQT/2E0HLSJSrHoI/nGdDlpEpGBRBP96mg5a9wGIFGF7Y4LHj57IXyfuQ9uGLcx88nlaOndE3axYiCr4N904d/eMoBZOB70diro5LNt00IuXPdNnm+zTQb+pda+Cj6sEIFKgjtHDuXLO6bwxIsn24Y0M37mTX854C9/95k8ZuX5k1M2ra1F1+ww0HXQxCSB200GL1Lo7zzqSjtHD2T68EYAdjY10Dh/OTe8/nbbUi9E2ro5F2eev6aBFBICnpo4nnexzq2lDA38bN47hTZ3RNKrORT3gq+mgRQSAZCqddbnhJNPZ10nhog7+UH/TQWsMQKRAp/7peX518lvY0di4e1kyleLElX9jxK5d+usqoWoI/hAM9G6HskwHffqJJ3H6iSf1Wva5T/zb7u+bhg/nR1+5tejj9KSPqEiB/vG3f+GlCSN59uDxmAdn/uM6NvPl/7ybjewTdfPqRrUE/wxNBy0iDEt3c+XtT/LG2C7WtY2k7bXXeceqv/Ma+9CenBh18+pCtQX/eqMEIFKkUetHMGm9A3vxTLLfQ5ekQAr+5adBYBGpOgr+laEEICJVRcG/cpQARKRqKPhXlhKAiFSFuAf/WE0HbWZNZvakmT1jZsvN7Pqo2iIi0Yp78I/jdNA7gBnufiQwDTjDzI6LsD0iEoFaC/71NB10ZAnAA5kJU4aFX6Wf7k5EqlYtBv+mG+fS8MpazJ2GcDroYpNAtumg125Y32eb7NNBFyPSMQAzS5jZUmAD8Ii7/ynLNrPNbLGZLd5EqvKNFJGyqLXgDwNPB12MWE4H7e5pd58GtAFvN7OpWbaZ5+7T3X36XrpvrSZ1egOveZJuXd9JqCMRnMzVUvAHTQddFu7eASwEzoi4KVJCnd7AD1PN3JQez9fT4/hKeiyvprZE3SyJWEe6CXa01FzwB00HXTJmtq+ZtYbfjwBOBf4aVXuk9H6cbmUlLeyyBDssySYbwTeYAqkNUTdNItKRGgWeqMngD5oOupT2B35oZgmCRHS3u98fYXukhNb5MNYxkpT1fmDKLhr4HftwSkTtkujUYp9/X5oOukTc/VngqKiOL+XVQZKGLINW3dbAOh8FdFW+URKd8AE5tRz8M+ppOuiqGAOQ+tPGTnZl+Xg1eoqjUBdQnHQ2pOjw5qL7q6X0lACkLMZYmhmso8n3lO4mPc1odnE8myNsmVTS5kSK1M5gwDeRyL+9VJbqKqVsZiRSTEyv4hEfRyfDOI51nMl6OpJtUTdNKqAjkarZap+4UAKQsjGDQ5JNHELmbsUkWxkfaZukMjrSTZCq3WqfuFAXkIiUXg2XesaJEoCIlFSm3FMG75Lrr+bgf3gn77gwe3WRu/OZr32RI887jeMvOoelf11ekuMqAYhIaaTTdVHrH4WLZ53Pvd/8Xs71D//+cf6+ZjVL7/sNt15zA3NuKs3s+UoAIlK8dHp3qWe9B/+72+fztgUzaLl/Mm9bMIO724ufDvqEo49lr+aWnOsffGwB7zvrXMyMtx8+jc1bt/DKxuLLqZUARKQomTr/OJR63t0+n8ufm8uarrU4zpqutVz+3NySJIGBrH11PW3j9sw3NH5s/+miC6EEICJFydT5x8H1K2+hK917Ouiu9HauX1ncdND5ZHvuSylurFMCEJGCxW3At70r+7TPuZaXyvj9xtL+yp5jvLy+/3TRhVACEJGCxHHAt21E9mmfcy0vlTNPmsFPH/wl7s6Tzy2lefQYxu1TfALQjWAiMmRxDP4A1x02h8ufm9urG2hEoonrDituOuiPXv0pFi15itc6NvHWs07i6tmXsysVTKPysfdexOknnMTDv3+cI887jZFNTXznui8VdbwMJQARGZK4Bn+AC9uCOv3rV95Ce9c62kbsz3WHzdm9vFD/8aWbB1xvZtz82WuLOkY2SgAiMmhxDv4ZF7bNKjrgVwuNAYjIoCj41x8lABHJS8G/PikBiMiAFPzrlxKAiOSk4F/flABEJCsF//qnBCAi/Sj4V5amgxaRqqDgX3maDlpEIqfgn98Dq7dw2gMvcsTPVnHaAy/ywOotRe8zdtNBm9kEM/udma0ws+VmdkVUbRERBf/BeGD1Fj6/5FXWbUvhwLptKT6/5NWSJIGB1ON00Cng0+4+GTgOuNTMpkTYHpHYUvAfnFuXvc72dO+5mbennVuXvV7W49bddNDuvs7dnw6/3wqsAMZH1R6RuFLwH7xXtqWGtLxU6no6aDObCBwF/CnLutlmttjMFm+ivG+ySNwo+A/NuJHZp0/LtbxU6nY6aDMbDfwc+KS79+tIc/d5wDyAKTYqy4WQiBRCwX/orpi6N59f8mqvbqCmhHHF1L2L2m8sp4M2s2EEwf9Od783yraIxImCf2Hec1AzEIwFvLItxbiRSa6Yuvfu5YWK3XTQFoxg3AGscPeBf3sRKRkF/+K856DmogN+tYhyDOAE4IPADDNbGn6dFWF7ROqegr/0FNkVgLsvAvQpFKkQBX/pqyqqgESkvBT8JRslAJE615FuAhT8pb/Iy0BFpHx05i8D0RWASJ1S8K8dmg5aREpGwb+2aDpoESkJBf/yurt9Pm9bMIOW+yfztgUzuLt9ftH7jN100CJSepsTwfQBCv7lcXf7fC5/bi5rutbiOGu61nL5c3NLkgQGUo/TQYtICW1OpPAdLQr+ZXT9ylvoSm/vtawrvZ3rV95S1uOWazpoVQGJ1IGORAoU/MuuvWvdkJaXSl1PBy0ihetIjVLwr5C2EfsPaXmp1O100CJSOA34VtZ1h83h8ufm9uoGGpFo4rrD5hS131hOBy0ixVPwr5wL24I6/etX3kJ71zraRuzPdYfN2b28ULGbDlpEipBO0+HNJRkIlKG5sG1W0QG/WigBiNSYzoYUqVQLZkYiEXVrpPo4qfTgHp+rBCBSQ1TqWQbejeNYjc1O71lqQx2Hhi7swEUc83LrnhU58oESgEiNUKlneXSvWsfmvd9ES3J41SaBbMEewBo7ARi5K4HjvJ5KM3rlaib1DP4DUAIQqRUK/mWx7Qt3sWHuRWyctD9Y1JXx2QM9iR00AMn0AO3zbhpXvcA+X/jqoI+mBCBSAzLlnlIGmzrZ9qnbK37YXP307+KPAHQlRpe9DUoAIlVOtf61baAB2emJZf2WdVH+wJ+hBCBSrcJST1Dwr3bpdPCv5xhtzRboq4ESgEg16lHnr1LP6pJKp8naVx8OyE5Pv1jR9hQjbwIws2ZgX3f/e5/lR7j7s2VrmUiMZc78Ffyjlav7pl+ZJUC6Ag0qsQETgJldCHwd2GBmw4CPuPtT4eofAEcXc3Az+z5wNrDB3acWsy+ReqE+/8rLFeiTExYBMG1tn2A/yDLLapfvCuBq4Bh3X2dmbwd+bGZXu/u9UJKC2R8A3wJ+VIJ9idQ8Bf/ySqdz99O3jP09R6xP9K6+6Rv460y+BJBw93UA7v6kmZ0C3G9mbeQsWB08d3/czCYWux+ReqDgX1pDLrPc2EJXzLrc8iWArWZ2SKb/P7wSOBn4BfC2cjdOJC4U/AuXc1CW6Mssq12+BPCv9OnqcfetZnYGcGHZWtWDmc0GZgOMo7EShxSpKAX//AYss2zsrKnKm2qSLwG8AYwFVvVZfhyE11Fl5u7zgHkAU2xU0d1OItVEwb+/VCba9zmrtwMXYVAX1TfVIl8C+DrBQHBfXeG6+pgUWyQCCv5DrL6pk8qbapIvAUzMVuvv7otLMXhrZj8FTgb2MbN24Dp3v6PY/YpUu7gF/4GmQ2gZ+3sO3djSe2GdV99Ui3wJoGmAdSOKPbi7v6/YfYjUmnoP/oVU30g08iWAp8zs4+7+vZ4LzexjwJLyNUukPtVT8M95Vp9jUFbVN9UnXwL4JHCfmV3MnoA/HWgEzi9nw7JZYW+p9CFFSqYWg39Bk5xpULZmDJgA3H098M7wBrDMVA0PuPujZW9ZDkcnHubp9GlRHV6kILUQ/Aea5MzGLe1ffSM1L99cQE3AJ4BJwHPAHe4+uKcNl0HCjTRKAlJbqi345wr0dmBQeZO1zFLBvy7l6wL6IbALeAI4E5hM0C0UmdZ0ko5ESklAakLUwV9lljKQfAlgirsfDmBmdwBPlr9J+WWSwLFXLeapm6ZH3RyRrCoZ/FVmKYXIlwB2Zb5x95RZdVzCQpgEvvo6V1y8L7feeVTUzRHppZzBX2WWUir5EsCRZrYl/N6AEeHPBrh7+NSKiLSmkzxx16uAuoKkepQy+OcK9prkTEohXxVQTUyOqvEAqRaFBP98c9T3674RKZGafyawBoWlWuQL/rkmORvwWbIK/lJGNZ8AQElAotcz+A90Rm8HLuLEl5K9++l145REpC4SACgJSDQ60uF0WRZE8VQYzAd6lmzcnjol1atuEgCUNwnYMGf0O1PYcOj8Q5LurdVTESXltzndlPMZqC1jF3Hoxj6BXmWWUgPqKgHAniQwKb2SVYnDSrLPkcekOOQ/ttKQCEPAMKPzqi7+/vMDSrJ/qS67z+r7OIXfB98kglP4TZlun77BX6RG1F0CgCAJNDeuLslVgDU5h/xoKw3NDb2Wj75pBAc9/RKrXziw6GNIdHIG+0S2B97t6bvZlEhBqnqmdxApRF0mgIxSdAU1n5KiwbJc/CeN1v+RZvWXi9q9VEBnegSpHJOckejklHSWGS0HsCndBKmEgr/UvLpNAKUaD2gY7dCQ5Q99mEGzRvOqyebwbL5/qHfswEWc/HKfj3sB1TebqmxiN5Fi1G0CgNIkgc5FyZ5X/j1WdMOvO4troBQsV9dNpvrmXWv7fLT7Bv8CKPhLvanrBADFJ4Fd6xrY9q0djLxkODRZcDXQ2Q1/7OK1343MnhykZHIFeho7ad1rKUdt7PMR7hv4S2RT2A4Ff6kn5p6ruK36JO1oH5N8rKDXdiSCG3MKvRKYeEw7Le93GGnwi05ee6iJ9oaJBe1Lshts9U2lbW1IkdrZouAvNas51bHE3ftNnVz3VwAZxZaHvrikrfdTkBtybip55DyrJ3/1TaV1JFL4DgV/qU+xSQCwpzz0XRdpCuly60yPAMhefUOuQF9dNiVSoOAvdSxWCSBgmkK6xLLfJeu7JzkbapllNVCpp8RBpAnAzM4AbiW4xr/d3cteVd+aTmjOoCLk6r4pVZll1XAFf6l/kSUAM0sA3wb+AWgHnjKzX7n7X8p9bE0cl18UZZbVIlPuKVLvovyrfTuwyt2fBzCzu4BzgbInAFAS6ClXsG8du4ij1lvv6psylVlWC9X6S5xE+dc8HljT4+d24B19NzKz2cBsAGNCSRsQtyQwpEAPsDHHTXD1KJ1mkzdjZlFVm4pUXJQJINspVr+xRHefB8yD4D6AUjeiHLOHRi3n3DfkqL6JU6DPYmtDilRK1T4SP1EmgHbodUrfBqyNoiHJnWNqrjx04DJLD26e0qnsoOgmL4mrKBPAU8ChZvZm4GXgIuD9UTRkdKIL0smqLQ/NPslZvjJLBf/B0ICvxFlkCcDdU2Z2GfAbgmj1fXdfHlV7MqIeDxiozDLpfapvarnMsgpowFfiLjZzAQ1WsXMGDfo4A0yHkJywqH+ZpZSUgr/ESeznAhqsclQGDbn6RsG/rBT8RQKKNFkUmgQGnOQs26BszKtvoqDgL7KHEkAOmSRwxcV/7lcZNOQyS0CRPnoK/iK9KQEMwHa08MRdrzIx9WaW2uF7VjRuBWpzkrO4UvAX6U8JgFyzWQKWxsYtZe9XXsDT++1ZruqbmqLgL5Jd7BJAvknOTswyydnCBFhiQ+8kIDVBwV8kt7pNAAMNyLaOXcS0ITxL9uT0MhYmpioJ1BgFf5GB1UUCyBXsT871LNm+wX8QlARqi4K/SH41lwByBvsKPEtWSaA2KPiLDE5NJYA0RnLCov799BWUSQKk05psrQop+IsMXkPUDRiK0bwRafDPaN1pWONrQRKQqqHgLzI00UfTGjFt8hzunbmENS0wYTN8ZMEx2N9eU1dQlVDwFxm6mroCiMq0yXOYN2sJL7WCG7zUCvNmLeFDbzkfS2yIunmxp+AvUhglgEG4d+YStjX2XratERbODGYmVRKIjoK/SOGUAAZhTUvu5SeH00EoCVRez+B/7+QtTL2sg5ZrOph6WQf3Tt4ScetEqp8SwCBM2DzwciWByusb/C+Z1d2ri+6SWd1KAiJ5KAEMwgULjmHkzt7LRu4MlmcoCVRO326fa2d2Z+2iu3Zmd6WbJlJTlAAGYemKW5g9/xgO7ABzOLADZs8/hqUrbum1XSYJqDy0fLL1+Q/URSciuakMdJCWrriFg1fAwZmfc2yXuUdA5aGll2vAd8LmoNunr1xddyIS0BVAiU1LPAeoK6jUBqr2uWFBQ9YuuhsW6OMtMhD9hZSBxgNKK1+p5wUrmvnO/IZeXXTfmd/ABSuaK9lMkZpj7tkfbViNxthbfXrye1E3Y9AWJqYCqDuoCKrzFylec6pjibtP77tcVwBlpCuB4ij4i5RXJAnAzP7JzJabWbeZ9ctK9URJoDAK/iLlF9UVwDLgAuDxiI5fUSfr4fFDouAvUg1WLYgAAAl1SURBVBmRlIG6+woAs/j8gQfloXqQTD5bG1KAgr9IJVT9GICZzTazxWa2eBcdUTenYCoPza8jkSK1s0XBX6RCypYAzOy3ZrYsy9e5Q9mPu89z9+nuPn0YWe72qSH1Ph7whid5yvdjue9N9xCLyzYlUvgOBX+RSipbF5C7n1qufdeyWniu8J2TU1wzE15qgQM3w40L4OIVA39U7u8+iNu6DydBN44xml3cxBMclOzKe7xN6SZIJRT8RSqs6ruA6lE1XwncOTnF7FmwOpxZc3UrzJ4VLM9lpbdyW/fh7LAk26yRLhvGq4zgs7ybdCr36yAc8HUFf5EoRFUGer6ZtQPHAw+Y2W+iaEeUqjUJXDOTrDNrXjMz92vu757Izr4fJTO2kWQZ++Q9poK/SDQiSQDufp+7t7n7cHcf6+6nR9GOqFVjeehLOWbQzLUcYDPDcev/UTKgk8b+LwBIp9mUGhWrSjCRaqMuoIglPbwKqJIppA/MMYNmruUAJ9g6mrx/V0+KBg5nY7/lWxtSbPJmzIxEotCWikixlAAidmL3st1TSFeDGxeQdWbNGxfkfs0p1s6BbGF4mATMneGe4kMsp5neO+tZ6qngLxItPQ+gCkxLPMdCqqMyKKj2GVoVUKN1c0tiEb9NH8Dj3sZodjKL5zmSjZDc87pNiRSo1FOkamg20CpS77OHbkqNUvAXiYBmA60BmUHhhvEPR9yS0svM7yMi1UMJoMqcnF6GvzJtSOWhd05OMfGyFA3XBP8OVLMfBU3uJlKdlACq0FDKQwu5catiwlJPUPCX+Ll38hamXtZByzUdTL2sg3snb4m6Sf0oAVSpwZaHFnLjVqVkSj0V/CVu7p28hUtmdfNSeGL2UitcMqu76pKAEkCVGmx5aCE3blVC5sxfpZ4SR9fO7M56YnbtzO5oGpSDEkAVG8wU0oXcuFVu6vaRuFuT4wQs1/KoKAFUuXxzBhVy41Y5KfiLwIQcJ2C5lkdFCaAGDJQELl6RZN58OKgDzIN/583PP31zOSj4iwRuWNCQ9cTshgXVFXJ1I1gNqeYbxRT8RXq7d/IWrp3ZzZqW4Mz/hgUNXLCiOZK25LoRTAmgxixMTCV55d7suql6ZvFQ8BepbroTuE607jRSX3096mbspuAvUruUAGrMtMRzQXloFTxIRsFfpLYpAdSgwZSHlpuCv0jtUwKoUVE+UlLBX6Q+KAHUsCiSgIK/SP1QAqhxlZxCWsFfpL4oAdSBzBTSe326fDOAKviL1B8lgDqRdOj4ennKQxX8ReqTEkCdOLF72Z4ppEtIwV+kfkWSAMzsa2b2VzN71szuM7PWKNpRb07sLu2gsIK/SH2L6grgEWCqux8B/A24KqJ21J1SVQYp+IvUv0gSgLs/7O6ZEcs/Am1RtKNeFZsEFPxF4qEaxgD+GXgo10ozm21mi81s8S46Ktis2lZoeaiCv0h8lC0BmNlvzWxZlq9ze2xzDZAC7sy1H3ef5+7T3X36MDRUMBSZ8tDBXgko+IvES9nmFHb3Uwdab2YfBs4GZnotzUldY05OL9v9HIGBKPiLxE9UVUBnAJ8FznH3bVG0IU52l4em01nXK/iLxFNUYwDfAsYAj5jZUjO7LaJ2xMKJ3cswwBpf67dOwV8kviJ5rJS7T4riuHF2UtgVZIkNux8pqeAvEm/VUAUkFdKzPHRTIqjCVfAXiS8lgJjJJIF9r1TwF4k7JYCY6UiNYtqOF0h/9XWax98bdXNEJEJKADHS0aPPf3r6RXhlWrQNEpFIKQHEREeWAd+EG82J56NqkohETAkgBjobsg/4HtX9AmN2JpUERGJKCaDOdTakSO1syTnge1hiFYCSgEgMKQHUsc2JgYN/xvT0i4CSgEjcKAHUMd+RP/hnKAmIxI/V0jxsZvYqsDrqdpTIPsDGqBtRBfQ+BPQ+7KH3IlDK9+Egd9+378KaSgD1xMwWu/v0qNsRNb0PAb0Pe+i9CFTifVAXkIhITCkBiIjElBJAdOZF3YAqofchoPdhD70XgbK/DxoDEBGJKV0BiIjElBKAiEhMKQFEyMy+ZmZ/NbNnzew+M2uNuk2VZGZnmNlKM1tlZv876vZEwcwmmNnvzGyFmS03syuiblOUzCxhZn82s/ujbkuUzKzVzO4J48MKMzu+HMdRAojWI8BUdz8C+BtwVcTtqRgzSwDfBs4EpgDvM7Mp0bYqEing0+4+GTgOuDSm70PGFcCKqBtRBW4Ffu3ubwWOpEzviRJAhNz9YXdPhT/+EWiLsj0V9nZglbs/7+47gbuAcyNuU8W5+zp3fzr8fivBH/r4aFsVDTNrA94D3B51W6JkZs3Au4E7ANx9p7t3lONYSgDV45+Bh6JuRAWNB9b0+LmdmAa+DDObCBwF/CnalkTm68CVQHfUDYnYwcCrwH+E3WG3m9mochxICaDMzOy3ZrYsy9e5Pba5hqAr4M7oWlpx2Wapi21NspmNBn4OfNLdt0Tdnkozs7OBDe6+JOq2VIEkcDTwXXc/CngDKMsYWbIcO5U93P3Ugdab2YeBs4GZHq+bMtqBCT1+bgPWRtSWSJnZMILgf6e7x/VBzScA55jZWUAT0GxmP3H3D0Tcrii0A+3unrkSvIcyJQBdAUTIzM4APguc4+7bom5PhT0FHGpmbzazRuAi4FcRt6nizMwI+npXuPvNUbcnKu5+lbu3uftEgs/CozEN/rj7K8AaMzssXDQT+Es5jqUrgGh9CxgOPBLEAf7o7p+ItkmV4e4pM7sM+A2QAL7v7ssjblYUTgA+CDxnZkvDZVe7+4MRtkmidzlwZ3hy9Dzw0XIcRFNBiIjElLqARERiSglARCSmlABERGJKCUBEJKaUAEREYkoJQGQQzCxtZkvDu7h/ZmYjw+XjzOwuM/u7mf3FzB40s7eE635tZh1xn9lSqpcSgMjgdLn7NHefCuwEPhHexHUfsNDdD3H3KcDVwNjwNV8jqPEXqUpKACJD9wQwCTgF2OXut2VWuPtSd38i/H4BsDWaJorkpwQgMgRmliR4hsFzwFRAk5dJzVICEBmcEeFUDYuBlwjnahepZZoLSGRwutx9Ws8FZrYceG9E7REpmq4ARAr3KDDczD6eWWBmx5rZSRG2SWTQlABEChQ+v+F84B/CMtDlwOcJn2tgZk8APwNmmlm7mZ0eWWNFstBsoCIiMaUrABGRmFICEBGJKSUAEZGYUgIQEYkpJQARkZhSAhARiSklABGRmPr/2yqv7u93Ly8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJR3cBxurGx0",
        "colab_type": "code",
        "colab": {},
        "outputId": "43b869f9-f588-4979-f4c3-332830eb486c"
      },
      "source": [
        "# Visualising the Training set results\n",
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set0 = X_test, y_test0\n",
        "y_set1, y_set2 = y_test1, y_test2\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
        "plt.contourf(X1, X2, classifier0.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green','blue')))\n",
        "plt.contourf(X1, X2, classifier1.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green','blue')))\n",
        "plt.contourf(X1, X2, classifier2.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green','blue')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set0 == j, 0], X_set[y_set0 == j, 1],\n",
        "                c = ListedColormap(('red', 'green','blue'))(i), label = j)\n",
        "    plt.scatter(X_set[y_set1 == j, 0], X_set[y_set1 == j, 1],\n",
        "                c = ListedColormap(('red', 'green','blue'))(i), label = j)\n",
        "    plt.scatter(X_set[y_set2 == j, 0], X_set[y_set2 == j, 1],\n",
        "                c = ListedColormap(('red', 'green','blue'))(i), label = j)\n",
        "plt.title('Logistic Regression (Test set)')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
            "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
            "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
            "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
            "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5xbdZ3/8dd7kg7T6wzXlrZTQEG2WKDWwupPFlqKXLpcdxFBfruwLrLsD11QdpUKCAqsgquAgqtV+en6QxFRLqVFq9UC3RVlYIG2lrqVW6cXCnSmdOhlmszn98c56WSmSSYzk+ScJJ/n4zGPTs45OfkkneST7+V8vjIznHPOuXwaog7AOedcvHmicM45V5AnCueccwV5onDOOVeQJwrnnHMFeaJwzjlXkCcKV3aSLpS0eIj3XSlpVolDij1Jj0q6qEznPlnSg+U4dxQkLazHv5FKkl9H4bJJehm4xMx+FcFjfw9oN7Nrh3meg4GXgLfDTW8A3zSzLw3nvLVCUhvwcWA98IesXaOBbUDmQ+E0M3tiiI+xETjXzJYNJ9Yc5/0SsJ+ZXZK17Xjgi2b2gVI+luuVjDoA58qoxcxSkmYCj0l62sx+WcoHkJQ0s1Qpz1lOko4Bms3syXDTmKx9BhxtZmsiCW7ongBaJR1pZsujDqYWedeTK5qkj0laI2mzpIclTczad7Kk1ZK2SPqGpMckXRLuu1jSsvB3SbpN0qbw2OclTZN0KXAh8GlJXZIWhMe/LOmk8PeEpM9K+pOkrZKeltQ6UNxm1gasBKZnxTtR0k8lvS7pJUn/lLVvpKTvS+qQtErSpyW1Z+1/WdJnJD0PvC0pOcD5jpXUJuktSa9J+mq4vUnS/5P0pqROSU9JGh/uW5r1+jVIulbSK+Hr9h+SmsN9B0sySRdJelXSG5KuKfBynAY8NtBr1u+1uF3SWkkbJX1d0l7hvgmSfh7G/qakX4fbfwIcACwO/y//Kcd5c9433Ncq6aHwubwo6bJw+9nAp4CLwvP+HsCCbpHHgLnFPi83OJ4oXFEknQh8ETgPOBB4Bbg33LcfcD8wD9gXWA38rzynOhk4HngX0AJ8GHjTzOYD9wC3mtkYMzsjx30/BVxA8IEwDvgoQVfJQLG/D5gGrAlvNwALgOeAScAc4EpJp4R3uR44GHgH8EHgf+c47QXAX4bPoWeA890B3GFm44B3AveF2y8CmoFWgtftMmB7jse6OPyZHcY0Briz3zHHAYeHj/05SVPzvBxHEvz/FOs2YHJ4v8MJ/t+uDvd9JjzXfgR/EzcAmNmHgE3AyeH/5ddynDfnfSUlgEXAfwETgVOBz0o6wcweBL4KfD8877FZ51sFHD2I5+UGwROFK9aFwN1m9oyZ7SRICu9XMB4wF1hpZj8Lu2G+BmzMc55dwFjgzwjGyFaZ2YYiY7gEuNbMVlvgOTN7s8Dxb0jaDvwW+AaQGcA9BtjfzL5gZt1m9iLwbeD8cP95wL+aWYeZtYfPp7+vmdlaM9texPl2AYdK2s/MurK6fXYRJIhDzSxtZk+b2Vs5HutC4Ktm9qKZdRG89udLyu46/ryZbTez5wgSVr4PzRZga95XLEt4/o8CV5hZp5ltAb7U73lNBKaEz/vxYs47wH2PA5rM7JZw+x+B/5v1mPlsDZ+bKwNPFK5YEwlaEQCEH1hvEnyDngiszdpnQHv/E4T7fk3wbfgu4DVJ8yWNKzKGVuBPg4h5P4Jv3/8MzAJGhNsPAiaG3R6dkjqBzwLjw/19nk+/33NtG+h8f0/wTfyFsHvp9HD7D4BfAPdKWi/pVkkj2FOf1z78PZl1fuibmLeRNfbQTwdBoi7GRILXbGXW83qQoFsJ4GaCAfHfKOiS/FSR5y1034OAg/u9lp8CJgxwvrFA5yAe3w2CJwpXrPUEb2IAJI0m+Da8DthA0D2R2afs2/2Z2dfM7L3Auwk+QP8ls2uAGNYSdN0ULfym/hVgB/B/ss7zkpm1ZP2MNbNMH3ef50OQoPY4db+48p7PzP7HzC4g+IC9Bbhf0mgz22VmnzezIwi66k4H/jbHY/V57YEpQAp4bRAvRcbzBK95MTaEj/POrOfVbGb7hs9ri5ldYWYHAX8NXCspM/Oo4P9lgfuuBV7I8VqeM8B5pxK0pFwZeKJwuYwIB1ozP0ngh8DfSZoeDmb+K/A7M3sZWAgcKens8NjLyfMNUNIxkv48/Ob8NsEHeDrc/RpBH3w+3wFulHSYAkdJ2rfI5/QlgoHyJuD3wFvhgPRIBYPk0xTMCIJgDGGepL0lTSKYSlpIwfNJ+t+S9jezHnq/9aYlzZZ0ZNgv/xZBd0w6x/l/BHxS0iGSxhC89j8e4myrRcAJxRxoZruAu4E7JO0Xvuatkj4YPq8zw5gEbAljL+r/ssB9M5Mersz87YX/zzOyzpu5X+ZcIhj3erTYF8ENjicKl8sigkHVzM8NZrYEuA74KcE3zXcS9hub2RvAh4BbCbqjjgDagJ05zj2OoP++g6AL5U3g38J93wWOCLsccl0Q9lWCD/HFBB+s3wVGFvmcFoaP+TEzSwNnEMyCeongOovvEAwsA3yBoOvsJeBXBAP1uZ4LELRaBjjfqQTdN10EA9vnm9kOgmR6f/hcVhHM3Pl/OR7iboJuqsfD8+8APlHk8+4f6zPAFkl/XuRdriRo0bQRfKD/HDg03DcVWEowPvA48G9Z4y83AzeH/5e5Em3O+4bJaS5BC+sV4HXg3+ntSrsXGAVslvRf4bbjgHVm9nyRz8kNkl9w50ounFXUDlxoZr+JOp7hkvSPBB/uRX0TjztJJwP/x8zOjjqWUpD0CMFg/68HPNgNiScKVxLhVNDfEbRA/oWg++kd4aygqiLpQIJuk98ChxG0Ru40s9sjDcy5iPiV2a5U3k8wjtFIUBbi7GpMEqFG4FvAIQRjCvcSTK91ri55i8I551xBPpjtnHOuoJrsemrQvtbQZ9p5bUjLmGp/jDoM51wE0jSggQ8bstV0vWFm++faF2mikHQ3wUVGm8xsWo79s4CHCKYEAvzMzL4w0HkbOIixyaLrnlWNzkSK33UfxJrE4VGH4pyroM7UaACSyfKliuNTS1/Jty/qrqfvEcwxL+QJM5se/gyYJGrduMa8/5fOuRpWziQxkEgTRVgIbHOUMVSTlnTQAJwx6daII3HOVUqmNRGlqFsUxXi/pOcULA357nwHSbpUQc3/th7eqGR8FdWSTsLG6QMf6JyrepXocipG3AeznwEOMrMuSXMJKlceluvAcD2D+QBJzaj5Ob+zr2rjN1+ZGXUYztUN23ssDdddjA6dBKrMB3cLglIOYVsPPWs2sO3Ge6Gjq+i7xTpRZNfmN7NFClZO2y+sLVS3WtJJOm/fTLAGkHOuEhquu5j9jz2KluQIVNb5R4GUJYDS5iTD2LLPvmy67ny2feo7Rd8v1l1PCpZLVPj7sQTxFlqopq7MSCyOOgTn6oYOnVSxJLH7MUv8UEI0J/ei4dADB3W/qKfH/ohgQZn9FKxJfD3h4jJm9k3gXOAfJaUIagidb34pORC2KhJDqTLtnBsSqWJJItOaKAch0ODaCJEminAxl0L772TPtYFdlhmJxTyT9i4o52pF5utfhYZBihLrridXWGa67BUX/nfEkTjnSsYSeZPEL//rCWb81akcffbJfPV78/fYv7O7m4vnfZKjzz6Z2Redxyvrc65IPGieKKqcLMET974edRjOuRIo1OWUTqe56pYv8NOvfZunfvII9/9iIS+8uKbPMf/x0P20jB3Hcw8u5vKPXMT1X/9KSeLyRFHlmnti1D51zu2WePQRmk4/iZHHTKPp9JNIPPpIweMza8jma020rXyed7RO4ZDJrTSOaOSvT57LwseW9Dlm4WNLuOD0YD2qs+ecwtLf/5ZSDOt6oqgJ8hlQzsVI4tFHaLz5eho2bkBmNGzcQOPN1xdMFlagywlgw6bXmDy+d7bSxAMmsH7Ta/2O2bT7mGQyybgxY9m8pZPh8kRRA1rSQXPVk4Vz8TDirtvRjh19tmnHDkbclXuRxGJmOeVqF6hfZrGcRw2fJ4oakRnYds5FT69tLHr7QF1OGRMPGE/7axt2316/aSMH7n9A3mNSqRRvdW1ln+aW4gPPwxNFjfGCgc5Fz8ZPKHq7WaKoqzPee8SRvLj2FV5e1073rm5+ungRc48/sc8xc48/kR898iAADy75BScc8749Wh1D4YmihnjBQOfiYdflV2JNTX22WVMTuy6/ss+2TGuimEyRTCb58r9cxzmf+HtmnvuXnHPSaUx952Hc9M2vseixXwPwt2edy+YtnRx99sncec/3uOHjVw3/yVCja2YnNcNqceGiYnQmUjRfuY8XDHSuxBoW3MJh++duKeSSePSRYKzitY3Y+AnsuvxK0qed3ueYctRzKsaa1zfQdcbn+2w7PrX0aTPL+cHhHds1xgsGOhcP6dNO3yMxZIsqSQyFdz3VKJ8B5VyMhR051ZAkwBNFTfIZUM7FW4ryFf0rB08UNcxbFc7FTzV1OWV4oqhRXjDQuRiqsi6nDE8UNcwLBjoXL9XW5ZThiaKGZQoGHppeHXEkzrlStCa8zLgrEzGu8ZWog3Cu7g23NeFlxl3ZZAoGHjOvLeJInKsvfcuMf5DkowuG1ZrwMuOurJLdY0nfujnqMJyrG3uWGV/PyH+9juSjC4Z8Ti8z7spqTGI74AUDnauUfGXG97rrtiGfs27LjEu6W9ImSSvy7Jekr0laI+l5STMqHWOt8IKBzlVO/jLjG3JuL0Y9lxn/HnBqgf2nAYeFP5cC/16BmGqaj1U4V375y4wfmHN7Meq2zLiZPQ4U6jw/C/gPCzwJtEga+itd51rSSR+rcK4C8pUZ33n5J4d8zijLjMe9KNAkYG3W7fZw2x7tN0mXErQ6EK0VCa5azUgs5pm0V5d1rlx2nnY6aWug6Ru3odc2YOMPZOflnyR12hnDOu8px53AKced0GfbtZf90+7fm/bai/+45Y5hPUYucU8UudpMOUdrzGw+MB+C9SjKGVQ1a0kn6UykODS9mjWJw6MOx7naZAnSc8/g7bnDSwxxEfUYxUDaoU/zYDKwPqJYaopfhOdceaQHPqTqxD1RPAz8bTj76X3AFjMb+rQBB3jBQOfKqdg1sKtJpF1Pkn4EzAL2k9QOXA+MADCzbwKLgLnAGmAb8HfRRFp7kt1jw4KBPlbhXKlkSojXWqaINFGY2QUD7Dfg8gqFU1fGJLbTCT5W4VyJZLqcqq2EeDHi3vXkysoLBjpXKrXY5ZThiaKOZQoG+kp4zg1PpbqcvMy4i4Svr+3c8FSqy8nLjLvIecFA54bGLPc6EwtfeYuTF77MUT9Zw8kLX2bhK28N63G8zLiLlBcMdG6I8qxat/CVt7jh6dfZsC2FARu2pbjh6deHlSy8zLiLBR+rcG5w8q1ad8eKzexI9/0mvyNt3LFi6LXW6rbMuIsPH6twbnAyA9i5xiY2bkvlvE++7cWo5zLjLma8VeFc8fINYE8YlfuLV77txajbMuMuXrxV4VxxUnkGsDOumLYPTYm+H9BNCXHFtH2G/JilLjOeSqf6/BSiUoyIx01SM2xs8rGow6hKnYngD8bLkDvXV8OCWzhs/wkFu5yyLXzlLe5YsZmN21JMGJXkimn78JcHjatApLllf9b/6Y0NdM79HH/BkwBsT4xhXKrzaTObmeu+/hXS9ZEpQ37Fhf/NHfe8J+pwnIulYnpz/vKgcZEmhsy4dvYAtxq7AGhkFzMTK9jOmKJO5V1Pbg+9BQOdc9kG6nKKAzMLfjBMaVAPo7Wd0drOqF0JRu0a/HPwROH2MCaxHQgKBjrnAj3hv3Er+rc7MYQ/wO7EMJqdjGbnsB/DE4XLSZbwgoHO9RGPDJErMSRGdJEY0bU7QZSaJwqXU3NP8Kbw6bLO9U7yiKw1YfRJDGrsQo29iaEplaApVb5uMR/MdnllBradq3s7m6lkiyLXbNTEiK7eZDCEcYbh8BaFG5AXDBy6sbN28a6fb2XaC1t41+KtjDtpV9QhuUHqTI0u+2Pk6k4ay9uM5e0+rYZLPn8DB35wDkef96G857nyy7dy+Nln8p7zz+OZF1aVJD5PFK4gLxg4dGPn7OLgb23jgcZODn3wJcY+vpoT56zkt+f72E+12BK2qJPJErcmjAG7k3rUQI/6fkT/7RlnsPDrd+Y97aP/+Z/8z9pXeeGBh/j3a67l8i9+sSTheqJwRfGxisGbeO0O7l33Fv/w+CZe7QqqiL76dop/edcbPDb15ajDc0Wwnc1DThL3tS/g3UtOpPmRqbx7yYnc175g2FNXj5/xXvYZ15x3/4LHlvI3c09HEu878ii2bN3KhjeGP9XdE4UbkJf2GJq9Du7hmt+/ybZU3/7mbT3GbXPeiCgqV6zhdDnd176ATyy/jrXb12MYa7ev5xPLr+O+dQtKPnU127rXNzF5wvjdtyeNP4B1m6o8UUg6VdJqSWskXZ1j/8WSXpf0bPhzSRRxuoC3KgZn12tibVfuyQBr838pdDEw1C6nTIvh86tvY3t6R59929M7uHH1bSWLMffj77mtFDO1IksUkhLAXcBpwBHABZKOyHHoj81sevjznYoG6XbLtCqOmdcWcSTVY+NX96J1dO7WWOuWCgfjBsV2NhdddbX/ILQau2jfviHnsWu3byxZjLlMPuAA2jf2Lma07rVNTNx//2GfN8oWxbHAGjN70cy6gXuBsyKMxw0g2T2W9K1DX3il3nTctxfzXt6bUf2qiI7qhk8u2S+iqNxAuhqC1kQiz3BBwSuhw7GG1pETct433/ZSOf2EE/jBokcwM55c/jzjxozhwP2qO1FMAtZm3W4Pt/X315Kel3S/pNZ8J5N0qaQ2SW09eP9vOWRKe1xx4X9HHEn1+PNvHcTN9+3LlC0ggymdcPOC/Thh1cFRh+bySHX37RdMpdNhGe78U1f7u+nwyxmVaOqzbVSiiZsOv3xYsV342Xkc93cXs/qVVzho7qnc/eCDfOv++/nW/fcDMPcDx/GOSZM4/OyzuOymm7jz6nnDeryMyMqMS/oQcIqZXRLe/hvgWDP7RNYx+wJdZrZT0mXAeWZ2Yu4z9vIy4+XTlR5JqnGrlyF3NSkYwBYSGL3jS8nWZYz5zqMctc/4/Hfu54fti7h29V2s3b6R1pETuOnwy/nI5LlliHrwXnj9daac8eE+2+JaZrwdyG4hTAbWZx9gZm9m3fw2cEsF4nIFjElspxOYfVUbv/lKzr8p56pSZ7oJlAbAwnLcM9MvBzvXt/BqenAdMB+ZPDc2iWG4okwUTwGHSToEWAecD3wk+wBJB5pZZlToTKA0lxm6YZEl2HL7ZsBbFa56daab9tg23ZYHM53SEQQUY5ElCjNLSfo48AsgAdxtZislfQFoM7OHgX+SdCaQAjYDF0cVr+vV3CM6E8F0We+CctWgKz2SdJ8lfEBTltEcXsZw8MZgXKLkV2DXiEivpDKzRcCifts+l/X7PKA0ozGupLxgoKsGva0GQ1OWIWDWqwqmNK0LP/7SaTrwJFGIX3LrhmXGpFt5Zt2now7DOSB3d9LsRLAu9O7E0G/aa4dFuFxplfBE4YasJZ2k0wsGugjlTAz8Z/BLvgshsnQkUpDy1sRAvNaTGzYv7eEqpSs9kq70SDrTTbuTRLJ1GcnWZcxOPBm0HhKJopIEAIO4AjsOvMy4q0peMNBVQiYxpDBSjVuhsWt3YviL9Un+Yv3g/w47wkRTbE6Jg6jKjPu73JWEz4BypVRwrKFUU1ctUdYup3JccHf8jPfy8vr1effnKzM+3DIenijcsGVmQB0zr42nvugX4bnB25Juon+NiJbxy3jPG+X5iOoo86p1P2xfxGXLb2JbWEH21e0buGz5TQBlvQgvX5lxTxQuFlrSSTpv9YvwXPGyWw27p66uy/pIKleSqMAA9rWr79qdJDK2pXdw7eq7ypooylVm3BOFK6krLvxv7rjnPVGH4WIo7wyl7GsaKmEYq9YVK1858WotM+6JwpVMsnssT9z7Ot6qcFDs1NXKjiSXu8spo3XkBF7NsSZFJcqMf+O+H/PhU07hdyuWl6zMuCcKVzKZgoGHplezJnF41OG4CutKjwQglTXakGxdBpA1Kym6KUaVvGbipsMv7zNGAaUrM/7Y00/zRmcnB809lesvvYxdqaBCwj+cey5zP3AcP//PZRx+9lmMamriO9ffMKzHy/BE4UpKlmBc4ys+A6pOdKVHZiUGg8YuNOHZ3rGGIUxbLZsKdDllZMYhSj3r6Z5/LTzdVRJf/0zpqx7F6H/R1QIvGFjb9uxOMpKty3pbDGkqO95QpM6wNVFJXmbcuQK8YGBt6T91tWV80J20e+pqnFoNeQxmDWy3p/j/D7uq5a2K6tW/5aApy3q7k8o0bbVcMgPY1XQFdtxU1/+4qxreqqguBa+Ehlh2JxVja0PwN+hF/4anOv/3XdXwVkU8DbfqarVIdXuXUyl4onBl462K+Ij71NVy8C6n0vHqsa7svAx5dMpRdbUa1GqXk5cZdzUpU4b8mHltEUdS+7LXacj8aErvOg2z0yuYnV4RdZgVkepujjqEsvAy465mecHA8tgSjjP0diYF1zSM6c6aulqlg9DDkg7qkEfdmqilMuORtigknSpptaQ1kq7OsX8vST8O9/9O0sGVj9KVyuyrvFVRCpnWggFMWdan1fAX65NlK81dLeKwBnamzPir2zdg2O4y4z9sX1TWx81XZny4IvuLkpQA7gI+CLQDT0l62Mz+kHXY3wMdZnaopPOBW4APVz5aN1zJ7rFsud1bFUNRcOpqPbYYCsgMYEfdmvAy46VzLLDGzF4EkHQvcBaQnSjOAm4If78fuFOSzHK9HC7OMgUDfbrswOpl6mq5RJ0koPbKjEfZ9TQJWJt1uz3clvMYM0sBW4B9c51M0qWS2iS19fBGGcJ1w+Xra+fWlR7JlqwBaAimrraM7+1SIpHwJDGASpUQL0a+cuKVKDP+g0WPYGY8ufz5migznivt928pFHNMsNFsPjAfIKkZ3uKIMW9VBHpbDkHV1dndz/UmgxqdtloucelyyvAy46XTDrRm3Z4M9B/OzxzTLikJNAObKxOeK4fMRXj1uGZFwbGGNN5iGKa4JAnwMuOl9BRwmKRDgHXA+cBH+h3zMHAR8FvgXODXPj5R/ephzYqu9EjSWJ/mr6Yso3kndT8rqdTi1OWUzcuMl4CZpSR9HPgFQe2Au81spaQvAG1m9jDwXeAHktYQtCTOjypeVzqZNStqUXZ3kqYsQ9BbddVnKJVcR2ZMJ0atiVoU6V+umS0CFvXb9rms33cAua9Vd1WvFsYqfOpqxCxRuSRhPRiGcg6dVg/DwHoGdR//S3aRqNaCgT51NT46KrxqXeOal9i8z77sk0xUbbIwjM2pNI1rXhrU/VSLXf5JzbCxyceiDsMNIJMo4tyqKK7qqotCR2p0RbucUnu38MZ1n6b70ENAVVomz3poXPMS+914K8mOzj67xqU6nzazmbnu5onCRaozkSLx6X146os5/z4j0X/pTxq7SI5/1hNDjMRtOmwtKJQo/C/fRSoOBQPzdidlupLS+HUNMdIZdjl5kqgc/+t3sTD7qjZ+85XKtCr2rLoKLeOD7qTeqas+3hBXtrPZk0SFeaJwkatUwcDslsMeU1f92oaqENdrJmrdgO8OSeOA/c3sT/22H2Vmz5ctMlc3ylUw0Keu1hbvcopOwXeLpPOA24FNkkYAF5vZU+Hu7wEzyhueqxelmC7rU1drm+1sRqWome0GbaCvVZ8F3mtmGyQdS3CV9GfN7GfkLtjn3LAMplWRb+pq39lJniBqQWYNbM/30RgoUSTMbAOAmf1e0mzgEUmTyVPF1bmhKqZgYFd6ZFZiCKquasKzvWMNPjupJtXqGtjVYqB31VZJ78yMT4Qti1nAg8C7yx2cq0fqUzBwj2sawnWhd7ca0vh4Q43zayaiN9A77B/p18VkZlslnQqcV7aoXN1S92issavPeMMeU1e91VA/0mnAk0TUBnrHvQ2MB9b02/4+4MmyROTqUp+BaMGJjQ9j6QOC2z51tW512LioQ3AMnChuJxjQ7m97uO+Mkkfk6sIeM5Qau5iVXhH8noKliWnBt0kfvaxb3uUUHwMlioNzXSthZm2SDi5LRK4m5Zq6Oit76mp6z/uo8c3eVoWrL97lFCsDJYo93929RpYyEFdb+s5OCiRblzGmG6YXUSZjVnoFSxPTGDEvxa4vetdTvfEup3gZ6B34lKSPmdm3szdK+nvg6fKF5apR31aD9e1OgkEPQs9Kr2DprZsBb1XUE+9yip+B3rlXAg9IupDexDATaATOKWdgLv5ydiclsuY45OhOGoq9r0rR8RVvVdQTTxLxUtR6FOGFdtPCmyvN7NdljWqYfD2K8ihUdXV6GWcmLU1M87GKOtGRbqrs8qZutyGvRyGpCbgMOBRYDnzXzKpv/Uo3LLmqrp5Q4aqrSmzyZFEPPEnE0kDv8u8Du4AngNOAqQTdUcMiaR/gx8DBwMvAeWbWkeO4NEGCAnjVzM4c7mO7gRWcuhrBVdCZgW1X27yEeHwN9K4/wsyOBJD0XeD3JXrcq4ElZvYlSVeHtz+T47jtZja9RI/p8hjK1NUoeKuidnV4CfFYGyhR7Mr8YmapEpb4PQuYFf7+fWApuROFK7FMxdU01jvWEK4JXezU1SjsblX4RXi1yVeti7WBEsXRkt4KfxcwMrwtwMyGPNl5fFZV2g2S8n1NbJLUBqSAL5nZg0N8vLrX22oIpq0CzOp+rrfFUAX1k5LmF+HVIu9yir+Cnw5mNuSvbpJ+BUzIseuaQZxmipmtl/QO4NeSlvdfaS/r8S4FLgUQrYOOt9YUnLqa6Uqqsm/mx/X4WEWt8S6n6lC2r5FmdlK+fZJek3Rg2Jo4ENiU5xzrw39flLQUeA+QM1GY2XxgPgTTY4cZftWJaupqpQkfq6gp3uVUFaL6BHkYuAj4UvjvQ/0PkLQ3sM3MdkraD/gAcGtFo6wC/aeuJg2OW1/ZqauVdEI4VuHJovpl1sB28RfVJ8mXgPvCUiCvAh8CkDQTuMzMLiGYivstST1AA8EYxR8iijc24iwSM9IAABDRSURBVDZ1NQo+XbY2+BrY1SOSTxYzexOYk2N7G3BJ+Pt/AUdWOLTYyTt1NTO+EJOpq1HwgoHVKzOAXWXDZHXL32Uxk6/q6nF9ZiX5u8sLBlavrQ1Bf1NmbGKTNbGoZzKvMJZ3aQunqZ0WdUcZouvHE0UM9Gk1NG5FE57tLZEBVTF1NSpeMLD6pLp7u5zW2Fg+esQBvDBnJTuat9O0ZSTzl0zlh3/YwIHaHnGkLqOoooDVJu5FAQesuuqK5gUDq0v/EuJnTZ3MY2esoqextw+1oTvBGQveyQ9W5ZwM6cpkyEUBXWnUy9TVqPgMqOrQv8tpl4nfzvlTnyQB0NOYZvGctbBqr4rH6HLzT6kyqrepq1HwGVDVI9Xd3Od2A8aO5tzdS9ubtwOeKOLCP61KqO9YQxck6m/qalS8VRFvnTmuwE4IWrY00tmy58D1vlv8/RIn/r8xRFvSTfQf3Um2LuO4tYpVxdV6kGlVzL4wxW/u8T/pOLKdzTm337JEXH5GA6nGnt3bRnQ3cMsSv74iTvxdVaRM1dXsqavJ1mCcYXd30vqkz1yNSNJg6b0+XTaOCq2BfcGqkYygh+vmGOubjYlbxI1LEpy7ygsFxonPehpA/3EGoO/UVRcbPgMqnjpSo72eUxXwWU+DUHDqqieIWPOCgfHjJcRrQ91/8vnU1dqRKRjopT3ioVCXk6sudflu6l8mw6eu1o6WbtHppT1iw5NEbaibT8T+ZTIAn7pag6YnlrOUaTRMWkzPupOjDqdueZdTbanZT8ic5bizl/50NWtWegVLN06POoy61RG+97w1UTtqMlGkEcnWZYzpzhpnSOM1jeuMFwyMiCVilSQ0pQH2ErYmzR4XP7miNEQdQDmM4W2OW5/0weg6Niu9gs7bN0cdRt3pSMRnyTodnODBHyR49z++xbgPv8m0m7by07N2Rh1WVarJROFchhJegbSi4rIGdgIe+GoDl63ZzKtvB1NXXt2e5vLpO7j//V6+fLA8UbiatXuyQtoHpSohTgPYiRMaufaFDral+/Y1bUsbN8zeFVFU1csThat5anwz6hBqXmeibwnxqGn/BtZuy90NtrahJ+d2l58nClfTMq2K2RfGp++8FllcupxC6Wd20To69xhla3d84qwWnihczestGOjKIU5dThn2P2lu1BhGJfomhVEN4oZFPsllsCJJFJI+JGmlpB5JOYtQhcedKmm1pDWSrq5kjKU0feonefHjx/PYNcfz4sePZ/rUT0YdUl05rsfHKsolbl1O2c6ZB99YO4opjQkETEk3cOdDSc5dEb/EFneRVI+VNBXoAb4F/LOZteU4JgH8Efgg0A48BVxgZn8Y6Pxj9Wc2M/nt0gY9RNOnfpL5ZzzNtsbebaO64dIF7+XZVbdFF1ideSwxDQMvGFhiHanRSPJLlGpAoeqxkbQozGyVma0e4LBjgTVm9qKZdQP3AmeVP7rS+tmcvkkCYFtjsN1VzgnhWIVPly2dzBXYniRqX5zHKCYBa7Nut4fbcpJ0qaQ2SW276Cx7cMVam3thr7zbXfnsni7rSsMSSPHrcnKlV7ZEIelXklbk+Cm2VZDrLzBvP5mZzTezmWY2cwQtQwu6DFq3DG67K7+GSYujDqHqZQawvTVRH8qWKMzsJDObluPnoSJP0Q60Zt2eDKwvfaTl9VdL3suofmvHj+oOtrvKm5VegXnBwOEJJwXEcQDblUecu56eAg6TdIikRuB84OGIYxq0Z1fdxqUL3suUTpDBlE4fyI4DH6sYug4bF3UIrsKimvV0DvB1YH+gE3jWzE6RNBH4jpnNDY+bC9wOJIC7zezmYs4fp1lPLp58fe2h8VXralfs1sw2sweAB3JsXw/Mzbq9CFhUwdBcHfH1tQfJu5zqll+i6OrSrHB9bVc873IaurSJH9shPGqtbCfJO9nCxxpWc6i2Rh1aUTxRuLrmrYrieJfT8NzVM5X7p+3khTm/ZEfzNpq2jOK3S6Zy7x/STNS2qMMbUJwHs50rKy8YODieJIam0xq5b1o3z5/xLDtatoFgR8s2nj/jWa4/omngE8SAJwpX17xg4MA6+q8/7wZlAyP5nzkr6WnsW2uspzHNwjnVMePfE4Wra14wsAgxWwO72kxgO9ubc6+q19W8o8LRDI0nClf3hC9ulE8cS4hXm73VTcuWxpz7Jm2pjgTsicLVPS8YmFtHjEuIV5t/W2I09lswqakbblxSHfOJPFE4hxcMzClmq9ZVs/NWjeabC5J9KjR8Y8EIzl1VHS226khnzlVIw6TF9Kw7OeowIuddTqV37qrRnLsq6iiGxlsUzoW8YGDAu5xcf54onOun7scqvMvJ9eOJwrks9T5WkVkD27lsniicy6FeWxW2s9lXrXN78EThXD+ZVsXeV9XXt2tftc7l44nCuRxaukXn7fVT2mNrgw9gu/w8UTiXw/TEcqB+Cgamur3LyeXnicK5PFq6VRcFA73LyQ3EE4VzeWRaFbU8VuFdTq4YniicKyBp1PRYRaq7OeoQXBXwROFcAZky5LU4XdZXrXPFiiRRSPqQpJWSeiTNLHDcy5KWS3pWUlslY3Quo5YvwvMk4YoRVYtiBfBXwONFHDvbzKabWd6E4lwlNExaHHUIJeNF/9xgRJIozGyVma2O4rGdG4paLBjorQlXrLiPURiwWNLTki4tdKCkSyW1SWrbRWeFwnP1phbGKrw14QarbOtRSPoVMCHHrmvM7KEiT/MBM1sv6QDgl5JeMLOc3VVmNh+YDzBWf2ZDCtq5AmalV7A0MS3qMIbFB7DdUJQtUZjZSSU4x/rw302SHgCOpbhxDefKRolNWPqAqMMYMk8SbrBi2/UkabSksZnfgZMJBsGdi0w1Fwz0Lic3VFFNjz1HUjvwfmChpF+E2ydKWhQeNh5YJuk54PfAQjP7eRTxOpdtVnpF1V2E15FuArw14YYmkjWzzewB4IEc29cDc8PfXwSOrnBozhVt9oUpfnNPlSw7bwlPEm7IYtv15FycVVPBwA5ftc4NkycK54YgUzCQdDraQIrha2C7YfJE4dwQJQ3U+GbUYRTkA9iuFDxRODdEcS8Y2JnwEuKuNDxRODcMcS4YaN7l5ErEE4VzJRC3goHe5eRKyROFc8MUt4KB3uXkSs0ThXMlEpexCtvZjORJwpWOJwrnSiAuYxWZLqdEIuJAXE3xROFcCcWhVeFdTq7UPFE4VyKZVsWIedFcCe0D2K5cPFE4V0Kz0itI3RpBaY/wCnFvTbhy8EThXBnMvrCyrYoOG1fRx3P1xROFcyVW6YKBvmqdKzdPFM6VWKZgYEUGtr3LyVWAJwrnyqBS02W9y8lVgicK58qonK0K73JyleKJwrky2d2qKOOaFZ4kXCV4onCujER51qzIrIHtXCV4onCujE4o11iFr4HtKiiSRCHpy5JekPS8pAckteQ57lRJqyWtkXR1peN0rlRKOVbhV2C7SouqRfFLYJqZHQX8EZjX/wBJCeAu4DTgCOACSUdUNErnSqCUM6A6vIS4i0AkicLMFptZ5tLVJ4HJOQ47FlhjZi+aWTdwL3BWpWJ0rtRK0qrwVetcBOIwRvFR4NEc2ycBa7Nut4fbcpJ0qaQ2SW276CxxiM4NTykKBnqXk4tK2RKFpF9JWpHj56ysY64BUsA9uU6RY5vlezwzm29mM81s5ghyDnk4F6nhFAz0LicXpWS5TmxmJxXaL+ki4HRgjpnlSgDtQGvW7cnA+tJF6Fw09r4qRcdXBvnW8y4nF6GoZj2dCnwGONPMtuU57CngMEmHSGoEzgcerlSMzpVDS7fovH1wrYrMGtjORSWqMYo7gbHALyU9K+mbAJImSloEEA52fxz4BbAKuM/MVkYUr3MlMZSCgb4Gtouacvf6VLex+jObmfx21GE4l9fSxDQsfcCAx3k9J1cp41KdT5vZzFz74jDrybm6NFCrYmuDD2C7ePBE4VwEiikYmOr2LicXD54onItIoYKBmS6nRKKCATmXhycK5yIyUMFA73JycVGTg9mSXgdeKcOp9wPeKMN5hyuucUF8Y/O4Bi+usXlcg5crtoPMbP9cB9dkoigXSW35ZgVEKa5xQXxj87gGL66xeVyDN9jYvOvJOedcQZ4onHPOFeSJYnDmRx1AHnGNC+Ibm8c1eHGNzeMavEHF5mMUzjnnCvIWhXPOuYI8UTjnnCvIE8UgSbpB0rqw6u2zkuZGHVM2Sf8sySTtF3UsGZJulPR8+HotljQx6pgAJH1Z0gthbA9IisWKV5I+JGmlpB5JkU+vlHSqpNWS1ki6Oup4MiTdLWmTpNItSl4Cklol/UbSqvD/8YqoYwKQ1CTp95KeC+P6fLH39UQxNLeZ2fTwZ1HUwWRIagU+CLwadSz9fNnMjjKz6cAjwOeiDij0S2CamR0F/BGYF3E8GSuAvwIejzoQSQngLuA04AjgAklHRBvVbt8DTo06iBxSwFVmNhV4H3B5TF6zncCJZnY0MB04VdL7irmjJ4rachvwaQosGRsFM3sr6+ZoYhKfmS0O1z0BeJJgFcXImdkqM1sddRyhY4E1ZvaimXUD9wJnDXCfijCzx4GhrS1bRma2wcyeCX/fSrCezqRoowILdIU3R4Q/Rb0XPVEMzcfD7oq7Je0ddTAAks4E1pnZc1HHkoukmyWtBS4kPi2KbB8FHo06iBiaBKzNut1ODD70qoWkg4H3AL+LNpKApISkZ4FNwC/NrKi4yrZmdjWT9CtgQo5d1wD/DtxIkIlvBL5C8CETdVyfBU6uRBy5FIrNzB4ys2uAayTNI1i58Po4xBUecw1Bd8E9lYip2LhiIldlwli0CONO0hjgp8CV/VrVkTGzNDA9HI97QNI0MxtwjMcTRQ5mdlIxx0n6NkGfe0Xki0vSkcAhwHPh+gWTgWckHWtmG6OMLYcfAgupUKIYKC5JFwGnA3OsghcVDeL1ilo70Jp1ezKwPqJYqoakEQRJ4h4z+1nU8fRnZp2SlhKM8QyYKLzraZAkHZh18xyKeJHLzcyWm9kBZnawmR1M8OaeUakkMRBJh2XdPBN4IapYskk6FfgMcKaZbYs6nph6CjhM0iGSGoHzgYcjjinWFHxb+y6wysy+GnU8GZL2z8zskzQSOIki34t+ZfYgSfoBwYwBA14G/sHMNkQaVD+SXgZmmlksShxL+ilwONBDUP79MjNbF21UIGkNsBeQWT3oSTO7LMKQAJB0DvB1YH+gE3jWzE6JMJ65wO1AArjbzG6OKpZskn4EzCIomf0acL2ZfTfSoABJxwFPAMsJ/uYBPhv1DElJRwHfJ/h/bADuM7MvFHVfTxTOOecK8a4n55xzBXmicM45V5AnCueccwV5onDOOVeQJwrnnHMFeaJwrkQkpcMKuSsk/UTSqHD7BEn3SvqTpD9IWiTpXeG+n0vqlFSxCzedGyxPFM6VzvawovA0oBu4LLz46gFgqZm908yOICi3Mj68z5eBv4kmXOeK44nCufJ4AjgUmA3sMrNvZnaY2bNm9kT4+xJgazQhOlccTxTOlZikJMH6DcuBacDT0Ubk3PB4onCudEaGJZzbCBaPirychHOl4NVjnSud7eEqfrtJWgmcG1E8zpWEtyicK69fA3tJ+lhmg6RjJJ0QYUzODYonCufKKFzj4hzgg+H02JXADYRrOkh6AvgJMEdSu6TIqsQ6l49Xj3XOOVeQtyicc84V5InCOedcQZ4onHPOFeSJwjnnXEGeKJxzzhXkicI551xBniicc84V9P8B8lEBrENz0ngAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OghQ5Bl3rGx3",
        "colab_type": "text"
      },
      "source": [
        "## LDA "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5xG2DEjrGx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train0 = y_train[:,0]\n",
        "y_train1 = y_train[:,1]\n",
        "y_train2 = y_train[:,2]\n",
        "\n",
        "y_test0 = y_test[:,0]\n",
        "y_test1 = y_test[:,1]\n",
        "y_test2 = y_test[:,2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ_boDByrGx7",
        "colab_type": "text"
      },
      "source": [
        "Since LDA is based on supervised learning, unlike PCA, which is based on unsupervised learning, we need to pass y in the LDA function as well , when we apply it to the dataset. So, this implies that we need to apply LDA to each training set separately. Let's go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTqbz6uIrGx7",
        "colab_type": "text"
      },
      "source": [
        "### Training Set 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expZp0aYrGx8",
        "colab_type": "code",
        "colab": {},
        "outputId": "4064122a-4678-4977-c74b-2ac5a3174cab"
      },
      "source": [
        "#Applying LDA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "lda = LinearDiscriminantAnalysis(n_components = 2)\n",
        "X_train = lda.fit_transform(X_train,y_train0) #Since it is a supervised model\n",
        "X_test = lda.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\devan\\anaconda3\\envs\\tesnor\\lib\\site-packages\\sklearn\\discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(2, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "C:\\Users\\devan\\anaconda3\\envs\\tesnor\\lib\\site-packages\\sklearn\\discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPYfPozHrGyD",
        "colab_type": "text"
      },
      "source": [
        "So here is the drawback of LDA, they explained it themselves. The number of components that are supposed to be selected should be not be larger than min(n_features, n_classes - 1) = min(13, 2 - 1) = 1 components. So in our case, it should be either set to None or 1 components. Let's try with 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUv89hknrGyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying LDA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "lda0 = LinearDiscriminantAnalysis(n_components = 1)\n",
        "X_train = lda0.fit_transform(X_train,y_train0) #Since it is a supervised model\n",
        "X_test = lda0.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mF_EnQTrGyG",
        "colab_type": "code",
        "colab": {},
        "outputId": "d264f63f-d9de-433b-8378-ecb955773512"
      },
      "source": [
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier0= LogisticRegression(random_state = 0)\n",
        "classifier0.fit(X_train, y_train0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJX4U8hCrGyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred0 = classifier0.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXt9soRUrGyJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "065d3d52-21d2-430d-d55e-34134a92fcf7"
      },
      "source": [
        "# if y_pred0 = (y_pred0 > 0.5), then the accuracy would have been..\n",
        "print('Test accuracy {:.2f}%'.format(classifier0.score(X_test,y_test0)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy 60.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVyGjs-CrGyL",
        "colab_type": "text"
      },
      "source": [
        "### Training Set 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaGMMK1FrGyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying LDA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "lda1 = LinearDiscriminantAnalysis(n_components = 1)\n",
        "X_train = lda1.fit_transform(X_train,y_train1) #Since it is a supervised model\n",
        "X_test = lda1.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnJB1aCOrGyR",
        "colab_type": "code",
        "colab": {},
        "outputId": "b3ee8157-548e-4d19-f57c-2e4c54ad92b5"
      },
      "source": [
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier1= LogisticRegression(random_state = 0)\n",
        "classifier1.fit(X_train, y_train1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_IGh6-7rGyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred1 = classifier1.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX-MKY8MrGyX",
        "colab_type": "code",
        "colab": {},
        "outputId": "a920bbd8-8115-4b7e-82df-bdc88259effb"
      },
      "source": [
        "# if y_pred0 = (y_pred0 > 0.5), then the accuracy would have been..\n",
        "print('Test accuracy {:.2f}%'.format(classifier1.score(X_test,y_test1)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy 20.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lxwha_orGyn",
        "colab_type": "text"
      },
      "source": [
        "### Training Set 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoMN08jfrGyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying LDA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "lda2 = LinearDiscriminantAnalysis(n_components = 1)\n",
        "X_train = lda2.fit_transform(X_train,y_train2) #Since it is a supervised model\n",
        "X_test = lda2.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE7LvIIlrGyq",
        "colab_type": "code",
        "colab": {},
        "outputId": "803f08f1-805d-4933-f881-0741bddafe88"
      },
      "source": [
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier2= LogisticRegression(random_state = 0)\n",
        "classifier2.fit(X_train, y_train2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9PaSOUQrGyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred2 = classifier2.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5N_3KiYrGy2",
        "colab_type": "code",
        "colab": {},
        "outputId": "bbb5030a-cec2-4bbf-e275-af15a9b8a2bf"
      },
      "source": [
        "# if y_pred0 = (y_pred0 > 0.5), then the accuracy would have been..\n",
        "print('Test accuracy {:.2f}%'.format(classifier2.score(X_test,y_test2)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy 80.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlxJAXG-rGzP",
        "colab_type": "text"
      },
      "source": [
        "### Combining and Reverse One-Hot Encoding the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrFG-2hcrGzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_pred = np.array([y_pred0.astype(int), y_pred1.astype(int), y_pred2.astype(int)])\n",
        "y_pred = np.array([y_pred0, y_pred1, y_pred2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn09EP6erGzW",
        "colab_type": "code",
        "colab": {},
        "outputId": "349972a5-af5b-4de0-a0c8-33b482c7a345"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 1.],\n",
              "       [1., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Js99FHtrGzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tranposing the y_pred so that it's shape remains same to that of y_test\n",
        "y_pred = y_pred.transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKZUVkQFrGza",
        "colab_type": "code",
        "colab": {},
        "outputId": "eb3449a9-9579-446b-a5d0-8f7fded5a8de"
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0.]\n",
            " [0. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCWLSXV9rGze",
        "colab_type": "code",
        "colab": {},
        "outputId": "1346fcb7-a3cf-4108-d3fb-9cc17896fb5d"
      },
      "source": [
        "#Printing a multilabel confusion matrix to check the results\n",
        "import sklearn.metrics as skm\n",
        "cm = skm.multilabel_confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print( skm.classification_report(y_test,y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[2 1]\n",
            "  [1 1]]\n",
            "\n",
            " [[1 2]\n",
            "  [2 0]]\n",
            "\n",
            " [[4 0]\n",
            "  [1 0]]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.50      0.50         2\n",
            "           1       0.00      0.00      0.00         2\n",
            "           2       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.25      0.20      0.22         5\n",
            "   macro avg       0.17      0.17      0.17         5\n",
            "weighted avg       0.20      0.20      0.20         5\n",
            " samples avg       0.20      0.20      0.20         5\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\devan\\anaconda3\\envs\\tesnor\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\devan\\anaconda3\\envs\\tesnor\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdZhd-28rGzi",
        "colab_type": "text"
      },
      "source": [
        "So, as we can see there are all zeroes present in the row of our first training example, this implies none of the values in the first example are above 0.5 . The machine might have predicted the greater one, and there must be a value of one of these categories higher than the others, but we can't know them , since lda rounded them off in its function. We can create our own lda with sigmoid, but since it is a tougher and longer process, and we already have a better solution, let's move ahead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVnleJf_rGzi",
        "colab_type": "code",
        "colab": {},
        "outputId": "faae7fcf-d1f8-48dc-e63b-afea998c4c21"
      },
      "source": [
        "# Converting the y_pred numpy array to list , for convenience\n",
        "y_pred_list = y_pred.tolist()\n",
        "print(y_pred_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0, 1.0, 0.0], [0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-_Kqna9rGzk",
        "colab_type": "code",
        "colab": {},
        "outputId": "03624deb-c7d9-41cf-dab5-1b3456ccada6"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jICocdJ2rGzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a categorised array with zeroes by equating it with the the newly created list \n",
        "# And initalizing it to the index which has value 1.0  \n",
        "y_pred_categorised = np.zeros(np.array(y_pred_list).shape[0])\n",
        "for i in range(len(y_pred_list)):\n",
        "    for j in range(len(y_pred_list[i])):\n",
        "        if y_pred[i][j] == 1.0:\n",
        "            y_pred_categorised[i] = j "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RUZpOs9rGzp",
        "colab_type": "code",
        "colab": {},
        "outputId": "9310ae28-b2c1-41ab-f3a5-46738a26b3d8"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHvHn4w5rGzs",
        "colab_type": "code",
        "colab": {},
        "outputId": "81ebf00a-32c4-4645-c5e6-bfd815fb1142"
      },
      "source": [
        "# Same process for y_test\n",
        "y_test_list = y_test.tolist()\n",
        "print(y_test_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv5GeHQ1rGzv",
        "colab_type": "code",
        "colab": {},
        "outputId": "ffd8b9dc-16ce-406f-beb3-d9da7955403c"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HMoOfNburGz2",
        "colab_type": "code",
        "colab": {},
        "outputId": "5ac69b96-2d7a-4955-c6af-b5772c5c9779"
      },
      "source": [
        "y_test_list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.0, 0.0, 0.0],\n",
              " [0.0, 1.0, 0.0],\n",
              " [0.0, 1.0, 0.0],\n",
              " [0.0, 0.0, 1.0],\n",
              " [1.0, 0.0, 0.0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkw8caTsrGz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_categorised = np.zeros(np.array(y_test_list).shape[0])\n",
        "for i in range(len(y_test_list)):\n",
        "    for j in range(len(y_test_list[i])):\n",
        "        if y_test[i][j] == 1.0:\n",
        "            y_test_categorised[i] = j "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utnZopGKrGz-",
        "colab_type": "code",
        "colab": {},
        "outputId": "2b33ab10-3344-4634-abc4-78b8fcc58230"
      },
      "source": [
        "y_test_categorised"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., 2., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFcrH-APrGz_",
        "colab_type": "code",
        "colab": {},
        "outputId": "1212b8b8-07f2-4638-8cd1-a8a9b917572d"
      },
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test_categorised,y_pred_categorised)\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 0]\n",
            " [2 0 0]\n",
            " [0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWsQq87crG0A",
        "colab_type": "code",
        "colab": {},
        "outputId": "55d13622-73e8-4a08-af36-3add458cddca"
      },
      "source": [
        "print(\"Accuracy:\",str(round((2+1)/(5)*100,2)), \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 60.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUufsLvqrG0G",
        "colab_type": "text"
      },
      "source": [
        "So, we got 3/5 examples correct. That is still great, since we don't have much training examples. bringing more will make this model better at predicting the virality of the websites correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c61ztAG_rG0G",
        "colab_type": "text"
      },
      "source": [
        "So that marks the end of this notebook. In the next one, we will fit this data using Logistic Regression , Kernel SVM, Naive Bayes and other models, without fitting them to each category separately."
      ]
    }
  ]
}